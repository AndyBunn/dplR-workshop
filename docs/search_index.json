[["index.html", "Learning to Love dplR Or Using R for Tree-Ring Analysis Chapter 1 Preamble", " Learning to Love dplR Or Using R for Tree-Ring Analysis Andy Bunn 27-June-2022 Chapter 1 Preamble R is both a programming language and a software environment for statistical computing. It is free and open-source, licensed under the GNU General Public License. The Dendrochronology Program Library in R (dplR) is an add-on package for R that performs many of the standard tasks in tree-ring analysis including cross-dating, detrending, chronology building, spectral and wavelet analyses, and so on. In this document, we will use example ring-width files from the ITRDB to demonstrate the functionality of dplR. The R environment is powerful and flexible and its use allows great transparency in presenting data results. An advantage of dplR’s open-source licensing is that one has the option to modify or add to the library’s functionality for performing specific experiments or producing custom figures. In the following pages we will cover the basics of dplR, crossdating, and some limited time-series analysis. Users should be familiar with the basics of dendrochronology and concepts like detrending, autocorrelation, spectral analysis and so on. If this is all new to you – you should proceed immediately to a good primer on dendrochronology like Fritts (2001) or the Cook Book (1990). These pages are not intended to teach you about how to do tree-ring analysis. They are intended to teach you how to use R for dendro. Please note! This is a very drafty document and there are typos and all kinds of silliness in it. This document was written in Markdown using the bookdown package. "],["introduction.html", "Chapter 2 Introduction 2.1 Before Starting 2.2 Layout 2.3 Getting Help with R 2.4 Citing R, dplR, and Other Packages", " Chapter 2 Introduction The R language and programming environment is now commonly used in dendrochronology. R is the world’s preeminent open-source statistical computing software and its power can be harnessed for tree-ring science through the contribution of add-on packages which are freely available on the internet. There are now many R packages for working with dendro data from measuring (measuRing) to standardization and chronology building (dplR, detrendeR), to fire history (burnr) to disturbance (TRADER) to climate-growth analysis (treeclim, pointRes, dendroTools, BIOdry) to working with data from dendrometers (dendrometeR) and cell anatomy (tracheideR, RAPTOR). Although extremely powerful, R has a steep learning curve that has led some to postpone using it in their own work. In these pages we will demonstrate the ways in which analysts can work with tree-ring data in R over the entire life cycle of a project in a transparent and reproducible way – from initial measuring of the wood to statistical tests to producing publication-quality graphics. These pages are written as a demonstration using on-board data sets but can easily be adapted for users to work with their own data. 2.1 Before Starting 2.1.1 R Install R by visiting www.r-project.org. We recommend that you use RStudio to interact with, and script in R. These documents were all made using R version 4.1.3 (2022-03-10). 2.1.2 Getting dplR Install the add-on library dplR. You can download and install dplRusing the install.packages function from the R prompt: install.packages(&quot;dplR&quot;) These documents use version 1.7.4 of dplR. You can check the version of your version of dplR via: packageVersion(&quot;dplR&quot;) If your version is older you can update it (and all your other packages) in R via: update.packages() These documents were all made using the most up-to-date versions of the packages available on the Comprehensive R Archive Network. Updating regularly is good practice! 2.2 Layout This document is laid out in chapters covering some of the more common where statistical software can helpful in in dendrochronology. Although focused on using dplR, we will make use of other packages throughout. If those are not installed on your system, you can get them from Comprehensive R Archive Network (CRAN) using install.packages. You only have to install a package once. After that it can be loaded with the library function as described in the next section. For instance, we will sometimes plot using ggplot which comes with the tidyverse bundle of packages. If we were doing time-series analysis we might use the signal package. If those aren’t on your R system you would install them via: install.packages(&quot;tidyverse&quot;) install.packages(&quot;signal&quot;) Note that you only have to do this one time. After the package is installed it remains on your system. At the of of each chapter we will include a list of packages that are used for that individual chapter. 2.2.1 A Note on Name Conflicts There are over ten thousand of R packages (aka libraries) available on CRAN. With that magnitude of user-developed content, it is inevitable that function names get reused. When two packages are loaded and have functions with the same name it creates a conflict. For instance, the function filter is part of the stats package that loads as part of R’s initial start up. It applies linear filtering to a time series. E.g., I’ll generate a time series (y) with a periodic component and we can plot it with a centered moving average (yFilt5). n &lt;- 100 y &lt;- 2 * sin(2 * pi / 10 * 1:n) + rnorm(100) yFilt5 &lt;- filter(x = y, filter = rep(1/5, 5),sides =2) plot(y,type=&quot;l&quot;,col=&quot;grey30&quot;) lines(yFilt5,col=&quot;darkred&quot;,lwd=2) However if we were to load the extremely popular dplyr package from the tidyverse we’d get different behavior from filter. Loading the tidyverse get several packages into this work space. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.6 ✓ dplyr 1.0.8 ## ✓ tidyr 1.2.0 ✓ stringr 1.4.0 ## ✓ readr 2.1.2 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Note that a list of conflicts is printed to the screen including dplyr::filter() masks stats::filter(). Now if we rerun the code above we get an error. yFilt5 &lt;- filter(x = y, filter = rep(1/5, 5),sides =2) ## Error in UseMethod(&quot;filter&quot;): no applicable method for &#39;filter&#39; applied to an object of class &quot;c(&#39;double&#39;, &#39;numeric&#39;)&quot; This is happening because both stats and dplyr have functions named filter and the filter from stats is being preempted by filter from dplyr. However, the error message is quite vague as to the cause of the error. But what is happening here is that filter is calling filter from dplyr and not from stats.1 We can still use the original function but we need to specify which filter we want to use. yFilt10 &lt;- stats::filter(x = y, filter = rep(1/10, 10),sides =2) plot(y,type=&quot;l&quot;,col=&quot;grey30&quot;) lines(yFilt5,col=&quot;darkred&quot;,lwd=2) So when you load packages watch for any warnings about conflicts! This book is laid out in chapters which are self contained to reduce this behavior. 2.3 Getting Help with R These pages demonstrate some basic aspects of tree-ring analysis through executable examples with on-board data sets. After a basic introduction, you will have a chance to work through examples yourself or work on your own analysis. No prior R experience is necessary but for those who are new to R, we suggest using the resources at YaRrr! The Pirate’s Guide to R to get started. 2.4 Citing R, dplR, and Other Packages It’s important to cite software for any number of reasons. E.g., being specific about version numbers you used will help track down discrepancies as software evolves. There is a nifty citation() function in R that gives you information on how to best cite R and, in many cases, its packages. citation() ## ## To cite R in publications use: ## ## R Core Team (2022). R: A language and environment for statistical ## computing. R Foundation for Statistical Computing, Vienna, Austria. ## URL https://www.R-project.org/. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {R: A Language and Environment for Statistical Computing}, ## author = {{R Core Team}}, ## organization = {R Foundation for Statistical Computing}, ## address = {Vienna, Austria}, ## year = {2022}, ## url = {https://www.R-project.org/}, ## } ## ## We have invested a lot of time and effort in creating R, please cite it ## when using it for data analysis. See also &#39;citation(&quot;pkgname&quot;)&#39; for ## citing R packages. As the citation function indicates: “We have invested a lot of time and effort in creating R, please cite it when using it for data analysis.” The creation of dplR is an act of love. We enjoy writing this software and helping users. However, we are not among the idle rich. Alas. We have jobs and occasionally have to answer to our betters. We ask that you please cite dplR and R appropriately in your work. This way when our department chairs and deans accuse us of being dilettantes we can point to the use of dplR as a partial excuse. There is more detailed information available in the help files and in the literature (Bunn, 2008, 2010). citation(&quot;dplR&quot;) ## Warning in citation(auto = meta): no date field in DESCRIPTION file of package ## &#39;dplR&#39; ## Warning in citation(auto = meta): could not determine year for &#39;dplR&#39; from ## package DESCRIPTION file ## ## Bunn AG (2008). &quot;A dendrochronology program library in R (dplR).&quot; ## _Dendrochronologia_, *26*(2), 115-124. ISSN 1125-7865, doi: ## 10.1016/j.dendro.2008.01.002 (URL: ## https://doi.org/10.1016/j.dendro.2008.01.002). ## ## Bunn AG (2010). &quot;Statistical and visual crossdating in R using the dplR ## library.&quot; _Dendrochronologia_, *28*(4), 251-258. ISSN 1125-7865, doi: ## 10.1016/j.dendro.2009.12.001 (URL: ## https://doi.org/10.1016/j.dendro.2009.12.001). ## ## Andy Bunn, Mikko Korpela, Franco Biondi, Filipe Campelo, Pierre ## Mérian, Fares Qeadan and Christian Zang (NA). dplR: Dendrochronology ## Program Library in R. R package version 1.7.4. ## https://github.com/AndyBunn/dplR ## ## To see these entries in BibTeX format, use &#39;print(&lt;citation&gt;, ## bibtex=TRUE)&#39;, &#39;toBibtex(.)&#39;, or set ## &#39;options(citation.bibtex.max=999)&#39;. The same practice goes for any other add-on package you might use. The filter function from dplyr acts much like subset from base R.↩︎ "],["using-dplr.html", "Chapter 3 Using dplR 3.1 What is Covered 3.2 Working with Ring-Width Data 3.3 Descriptive Statistics 3.4 Detrending 3.5 Descriptive Statistics for Detrended Data 3.6 Building a Mean-Value Chronology 3.7 Conclusion", " Chapter 3 Using dplR This document describes basic features of dplR by following the initial steps that an analyst might follow when working with a new tree-ring data set. The document starts with reading in ring widths and plotting them. We describe a few of the available methods for detrending and then show how to extract basic descriptive statistics. We show how to build and plot a simple mean-value chronology. We also show how to build a chronology using the expressed population signal from the detrended ring widths as an example of how more complicated analysis can be done using dplR. 3.1 What is Covered The Dendrochronology Program Library in R (dplR) is a package for dendrochronologists to handle data processing and analysis. This document gives just a brief introduction of some of the most commonly used functions in dplR. There is more detailed information available in the help files and in the literature (Bunn, 2008). In this document, we will walk through the most basic activities of working with tree-ring data in roughly the order that a user might follow. E.g., reading data, detrending, chronology building, and doing preliminary exploratory data analysis via descriptive statistics. 3.1.1 Load dplR We will be using dplR in here. Load it: library(dplR) ## This is dplR version 1.7.4. ## dplR is part of openDendro https://opendendro.org. ## New users can visit https://opendendro.github.io/dplR-workshop/ to get started. 3.2 Working with Ring-Width Data 3.2.1 Reading Data There are many ways that tree-ring data are digitally stored. These range in sophistication from the simple (and commonly used) Tucson/decadal format file of ring widths to the more complex (but richer) TRiDaS format. The type of data used most often by dendrochronologists is a series of ring widths from tree cores. We generally refer to these as rwl objects for “ring width length” but there is no reason these cannot be other types of tree-ring data (e.g., density). The workhorse function for getting tree-ring data into R is dplR’s read.rwl function. This function reads files in \"tucson\", \"compact\", \"tridas\", and \"heidelberg\" formats. The on-board rwl data sets in dplR (i.e., co021, ca533, gp.rwl) were all imported into R using this function. Throughout this document we will use the on-board data set ca533 which gives the raw ring widths for bristlecone pine Pinus longaeva at Campito Mountain in California, USA. There are 34 series spanning 1358 years. These objects are structured very simply as a data.frame with the series in columns and the years as rows. The series IDs are the column names and the years are the row names (both stored as characters). For instance, using the Campito Mountain ring widths we can load the data and learn some basic things about it: data(ca533) # the result of ca533 &lt;- read.rwl(&#39;ca533.rwl&#39;) nrow(ca533) # 1358 years ## [1] 1358 ncol(ca533) # 34 series ## [1] 34 We can look a little deeper at this object (ca533) and get the series names as well as look at the time values associates with the data: colnames(ca533) # the series IDs ## [1] &quot;CAM011&quot; &quot;CAM021&quot; &quot;CAM031&quot; &quot;CAM032&quot; &quot;CAM041&quot; &quot;CAM042&quot; &quot;CAM051&quot; &quot;CAM061&quot; ## [9] &quot;CAM062&quot; &quot;CAM071&quot; &quot;CAM072&quot; &quot;CAM081&quot; &quot;CAM082&quot; &quot;CAM091&quot; &quot;CAM092&quot; &quot;CAM101&quot; ## [17] &quot;CAM102&quot; &quot;CAM111&quot; &quot;CAM112&quot; &quot;CAM121&quot; &quot;CAM122&quot; &quot;CAM131&quot; &quot;CAM132&quot; &quot;CAM141&quot; ## [25] &quot;CAM151&quot; &quot;CAM152&quot; &quot;CAM161&quot; &quot;CAM162&quot; &quot;CAM171&quot; &quot;CAM172&quot; &quot;CAM181&quot; &quot;CAM191&quot; ## [33] &quot;CAM201&quot; &quot;CAM211&quot; head(time(ca533),n = 10) # the first 10 years ## [1] 626 627 628 629 630 631 632 633 634 635 3.2.2 Describing and Plotting Ring-Width Data Once a rwl data set has been read into R, there are a variety of ways to describe and visualize those data. Take note that this object is stored both as a generic data.frame in R but it also is part of a special class called rwl which will let R know how to do some special things with it like summarize and plot the data: class(ca533) ## [1] &quot;rwl&quot; &quot;data.frame&quot; Thus, we can plot a rwl object by showing either the segments arranged over time as straight lines or as a “spaghetti plot”. The rwl objects have generic S3 methods for plot and summary meaning that R knows how to do something special when plot or summary are invoked on an object with class rwl. E.g.,: plot(ca533, plot.type=&quot;spag&quot;) 3.3 Descriptive Statistics The simplest report on a rwl object can be print to the screen via: rwl.report(ca533) ## Number of dated series: 34 ## Number of measurements: 23276 ## Avg series length: 684.6 ## Range: 1358 ## Span: 626 - 1983 ## Mean (Std dev) series intercorrelation: 0.6294 (0.08593) ## Mean (Std dev) AR1: 0.7093 (0.09812) ## ------------- ## Years with absent rings listed by series ## Series CAM011 -- 1753 1782 ## Series CAM031 -- 1497 1500 1523 1533 1540 1542 1545 1578 1579 1580 1655 1668 1670 1681 ## Series CAM032 -- 1497 1523 1579 1654 1670 1681 1782 ## Series CAM051 -- 1475 ## Series CAM061 -- 1497 1523 1542 1545 1547 1579 1654 1655 1668 1670 1672 1782 1858 1960 ## Series CAM062 -- 1542 1545 1547 1548 1579 1654 1655 1670 1672 1782 1836 1857 1858 1929 ## Series CAM071 -- 1269 1497 1498 1523 1542 1547 1578 1579 1612 1655 1656 1668 1670 1672 1674 1690 1707 1708 1756 1782 1795 1820 1836 1845 1857 1858 1924 1948 1960 ## Series CAM072 -- 1218 1497 1498 1523 1533 1538 1542 1545 1546 1547 1571 1579 1580 1590 1654 1655 1668 1670 1672 1675 1690 ## Series CAM081 -- 1218 1336 ## Series CAM082 -- 1362 1858 1865 ## Series CAM091 -- 1655 1669 1670 1782 1858 ## Series CAM092 -- 1624 1654 1655 1670 1672 1675 1677 1690 1703 1705 1707 1708 1710 1733 1753 1756 1757 1774 1777 1781 1782 1783 1784 1795 1807 1824 1829 1836 1845 1857 1858 1899 1904 1929 1936 1961 ## Series CAM101 -- 1782 1783 1899 1929 ## Series CAM102 -- 1669 1690 1782 1858 1899 1929 ## Series CAM111 -- 1542 ## Series CAM112 -- 1542 ## Series CAM121 -- 1093 1218 1254 1361 1365 1460 1462 1468 1473 1475 1492 1497 1542 1544 1545 1547 1600 1899 1960 ## Series CAM122 -- 1117 1133 1147 1177 1218 1254 1361 1475 1497 1670 ## Series CAM131 -- 1361 ## Series CAM151 -- 1670 1703 ## Series CAM161 -- 1523 ## Series CAM162 -- 1618 1624 1641 ## Series CAM181 -- 1450 1523 ## Series CAM191 -- 1475 1497 1523 1533 1542 1558 1571 1578 1618 1655 1668 1670 1675 1677 1690 1705 1777 1929 ## Series CAM201 -- 1523 ## Series CAM211 -- 645 762 809 847 924 957 1014 1118 1123 1133 1147 1189 1350 1384 1468 1571 1641 ## 234 absent rings (1.005%) ## ------------- ## Years with internal NA values listed by series ## None That’s pretty basic information. We can look at some common (and not-so common) descriptive statistics of a rwl object: ca533.stats &lt;- summary(ca533) # same as calling rwl.stats(ca533) head(ca533.stats,n=5) # look at the first five series ## series first last year mean median stdev skew gini ar1 ## 1 CAM011 1530 1983 454 0.440 0.40 0.222 1.029 0.273 0.696 ## 2 CAM021 1433 1983 551 0.424 0.40 0.185 0.946 0.237 0.701 ## 3 CAM031 1356 1983 628 0.349 0.29 0.214 0.690 0.341 0.808 ## 4 CAM032 1435 1983 549 0.293 0.26 0.163 0.717 0.309 0.661 ## 5 CAM041 1683 1983 301 0.526 0.53 0.223 0.488 0.238 0.690 These are common summary statistics like mean, median, etc. but also statistics that are more specific to dendrochronology like the first-order autocorrelation (ar1), gini (gini), and mean sensitivity (sens1 and sens2). We would be remiss if we did not here mention that mean sensitivity is actually a terrible statistic that should rarely, if ever, be used (Bunn et al., 2013). Note that output object ca533.stats is itself a data.frame and its data can be used to plot, etc. For instance, we can look at the spread of the first-order autocorrelation via summary(ca533.stats$ar1) or make a plot to show the data. Here we will demonstrate a somewhat involved plot to get you an idea of how to layer plotting commands: boxplot(ca533.stats$ar1,ylab=expression(phi[1]),col = &quot;lightblue&quot;) stripchart(ca533.stats$ar1, vertical = TRUE, method = &quot;jitter&quot;, jitter = 0.02,add = TRUE, pch = 20, col = &#39;darkblue&#39;,cex=1.25) ar1Quant &lt;- quantile(ca533.stats$ar1,probs = c(0.25,0.5,0.75)) abline(h=ar1Quant,lty=&quot;dashed&quot;,col=&quot;grey&quot;) mtext(text = names(ar1Quant),side = 4,at = ar1Quant,las=2) Quick editorial note. I’ve switched from base R plotting to using ggplot in most all of my work. I need to go through and provide new plotting examples for everything, but time is short. Real quick though, here is a ggplot library(ggplot2) ar1 &lt;- data.frame(x=&quot;CA 533&quot;,y=ca533.stats$ar1) ggplot(ar1,aes(x,y)) + geom_boxplot(width=.2) + geom_jitter(width=0.1) + labs(y=expression(phi[1]),x=element_blank()) + theme_minimal() 3.4 Detrending Analysts often (but not always) detrend a rwl data set to create an object containing ring-width index (rwi) values. The dplR package contains most standard detrending methods including detrending via splines, fitting negative exponential curves, and so on. There are also dplR functions for less commonly used detrending methods like regional curve and signal-free standardization. A rwi object has the same basic properties as the rwl object from which it is made. I.e., it has the same number of rows and columns, the same names, and so on. The difference is that each series has been standardized by dividing the ring widths against a growth model (e.g., a stiff spline, a negative exponential, etc.). This gives each series a mean of one (thus referred to as “indexed”) and allows a chronology to be built (next section). As read.rwl is the primary function for getting data into R, detrend is the primary function for standardizing rwl objects (but see cms, rcs, bai.in, and bai.out as well). 3.4.1 Common Detrending Methods As any dendrochronologist will tell you, detrending is a dark art. In dplR we have implemented some of the standard tools for detrending but all have drawbacks. In all of the methods, the detrending is the estimation and removal of the low frequency variability that is due to biological or stand effects. The standardization is done by dividing each series by the growth trend to produce units in the dimensionless ring-width index (RWI). Much of the text that follows is modified from the help page of detrend. Probably the most common method for detrending is what is often called the “conservative” approach of attempting to fit a negative exponential curve to a series. In the dplR implementation the \"ModNegExp\" method of detrend attempts to fit a classic nonlinear model of biological growth of the form \\((f(t) = a \\times \\mathrm{e}^{bt} + k)\\), where the argument of the function is time, using nls. See Fritts (2001) for details about the parameters. If a suitable nonlinear model cannot be fit (function is non-decreasing or some values are not positive) then a linear model is fit using lm. That linear model can have a positive slope unless pos.slope is FALSE in which case the series is standardized by its mean (method \"Mean\" in detrend). For instance, every series in the ca533 object can be detrended at once via: ca533.rwi &lt;- detrend(rwl = ca533, method = &quot;ModNegExp&quot;) This saves the results in ca533.rwi which is a data.frame with the same dimensions as the rwl object ca533 and each series standardized. nrow(ca533.rwi) # 1358 years ## [1] 1358 ncol(ca533.rwi) # 34 series ## [1] 34 colMeans(ca533.rwi, na.rm=TRUE) ## CAM011 CAM021 CAM031 CAM032 CAM041 CAM042 CAM051 CAM061 CAM062 CAM071 CAM072 ## 0.9996 1.0000 1.0000 1.0000 1.0000 1.0012 1.0002 0.9999 1.0000 1.0000 1.0000 ## CAM081 CAM082 CAM091 CAM092 CAM101 CAM102 CAM111 CAM112 CAM121 CAM122 CAM131 ## 1.0000 1.0000 1.0000 0.9996 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 0.9998 ## CAM132 CAM141 CAM151 CAM152 CAM161 CAM162 CAM171 CAM172 CAM181 CAM191 CAM201 ## 0.9985 0.9999 0.9995 0.9999 1.0004 0.9994 0.9997 0.9998 1.0000 0.9953 1.0000 ## CAM211 ## 0.9998 When detrend is run on a rwl object the function loops through each series. It does this by calling a different function (detrend.series) for each column in the rwl object. But, a user can also call detrend.series and it is useful to do so here for educational purposes. Let us detrend a single series and apply more than one detrending method when we call the detrend function. We will call detrend.series using the verbose mode so that we can see the parameters applied for each method. The detrend.series function produces a plot by default. CAM011.rwi &lt;- detrend.series(y = ca533[, &quot;CAM011&quot;],verbose=TRUE) ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Verbose output: ## ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Options ## make.plot TRUE ## method(s)1 c(&quot;Spline&quot;, &quot;ModNegExp&quot;, &quot;Mean&quot;, &quot;Ar&quot;, &quot;Friedman&quot;, &quot;ModHugershoff&quot;, ## method(s)2 &quot;AgeDepSpline&quot;) ## nyrs NULL ## f 0.5 ## pos.slope FALSE ## constrain.nls never ## verbose TRUE ## return.info FALSE ## wt default ## span cv ## bass 0 ## difference FALSE ## ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Zero indices in input series: ## 1128 1157 ## ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Detrend by ModNegExp. ## Trying to fit nls model... ## nls coefs ## a: 0.66110 ## b: -0.01184 ## k: 0.31793 ## ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Detrend by ModHugershoff. ## Trying to fit nls model... ## nls coefs ## a: 0.45550 ## b: 0.15420 ## g: 0.01532 ## d: 0.32392 ## ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Detrend by age-dependent spline. ## Spline parameters ## nyrs = 50, pos.slope = FALSE ## ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Detrend by spline. ## Spline parameters ## nyrs = 304, f = 0.5 ## ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Detrend by mean. ## Mean = 0.4396 ## ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Detrend by prewhitening. ## Call: ## ar(x = y[idx.goody]) ## ## Coefficients: ## 1 2 3 4 5 6 7 8 9 10 ## 0.388 0.139 0.000 0.084 0.132 0.061 0.038 -0.126 0.037 -0.100 ## 11 12 13 14 15 16 17 18 19 20 ## -0.010 0.015 0.088 0.010 0.064 -0.013 0.015 -0.004 -0.054 0.124 ## 21 22 23 ## -0.030 -0.054 0.137 ## ## Order selected 23 sigma^2 estimated as 0.0209 ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Fits from method==&#39;Ar&#39; are not all positive. ## Setting values &lt;0 to 0 before rescaling. ## This might not be what you want. ## ARSTAN would tell you to plot that dirty dog at this point. ## Proceed with caution. ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Detrend by Friedman&#39;s super smoother. ## Smoother parameters ## span = cv, bass = 0 ## wt = default ## ## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ## Zero indices in Ar series: ## 993 Note that advanced users can use return.info=TRUE to have all the fitting parameters returned in a list. Having access to these is occasionally desirable. 3.4.2 Interactive detrending There are many instances where a user needs fine scale control while detrending perhaps fitting a negative exponential curve to one series, a spline to another, and the mean to a third. Like with crossdating, interactive detrending is a case where an app is useful. The iDetrend app is relatively simple interface to detrend.series that let’s users fit curves and plot them with several different options. When completed the user can save a file with the detrended data as well as generate log file that allows reproduction of the detrending from the command line. It’s available here andybunn.shinyapps.io/iDetrend/ 3.4.3 Other Detrending Methods There are other detrending methods that are less commonly used but might have theoretical advantages. These include regional curve standardization (function rcs), C-Method Standardization (function cms), and converting measurements of ring widths to basal area increment (functions bai.in and bai.out). See their help pages for further information. 3.5 Descriptive Statistics for Detrended Data It is also easy in dplR to compute commonly used descriptive statistics that describe the correlation between series (both within and between tree correlations) as well as the expressed population signal, signal-to-noise ratio, and subsample signal strength for a data set. These are done in dplR using the rwi.stats function so-named because these statistics are typically (but not always) carried out on detrended and standardized ring widths (rwi). If a data set has more than one core taken per tree this information can be used in the calculations to calculate within vs. between tree correlation. The function read.ids is used to identify which trees have multiple cores. ca533.ids &lt;- read.ids(ca533, stc = c(3, 2, 1)) rwi.stats(ca533.rwi, ca533.ids, prewhiten=TRUE) ## n.cores n.trees n n.tot n.wt n.bt rbar.tot rbar.wt rbar.bt c.eff rbar.eff ## 1 34 21 21 523 13 510 0.444 0.603 0.439 1.448 0.501 ## eps snr ## 1 0.955 21.09 There is (at least) one other way of looking at the average interseries correlation of a data set. The interseries.cor function in dplR gives a measure of average interseries correlation that is different from the rbar statistics from rwi.stats. In this function, correlations are calculated serially between each tree-ring series and a master chronology built from all the other series in the rwl object (leave-one-out principle). The average of those correlations is sometimes called the “overall interseries correlation” or even the “COFECHA correlation” in reference to commonly used crossdating software COFECHA. This number is typically higher than the various rbar values given by rwi.stats. We are showing just the first five series and the mean for all series here: ca533.rho &lt;- interseries.cor(ca533.rwi, prewhiten=TRUE, method=&quot;spearman&quot;) ca533.rho[1:5, ] ## res.cor p.val ## CAM011 0.5358 0 ## CAM021 0.6760 0 ## CAM031 0.5258 0 ## CAM032 0.6265 0 ## CAM041 0.4907 0 mean(ca533.rho[, 1]) ## [1] 0.6368 Again, if these concepts are unknown to you statistically look at some of the canonical works in dendrochronology like Cook et al. (1990), Fritts (2001), and Hughes et al. (2011). 3.6 Building a Mean-Value Chronology After detrending, a user will typically build a chronology by averaging across the years of the rwi object. In dplR the function for doing this is chron which by default uses Tukey’s biweight robust mean (an average that is unaffected by outliers). ca533.crn &lt;- chron(ca533.rwi) This object has the same number of rows as the rwi object that was used as the input and two columns. The first gives the chronology and the second the sample depth (the number of series available in that year). dim(ca533.rwi) ## [1] 1358 34 dim(ca533.crn) ## [1] 1358 2 An object produced by chron has a generic S3 method for plotting which calls the crn.plot function (which has many arguments for customization). Here we will just make a simple plot of the chronology with a smoothing spline (function caps) added. plot(ca533.crn, add.spline=TRUE, nyrs=20) The chron function is the most basic (and most common) way to buld a chronology but there are many other ways as we will see in the next few chapters. 3.7 Conclusion In general this page aims to give a very cursory overview of basic tasks that most dendrochronologists will want to be aware of. Know that we are just scratching the surface of what dplR is capable of. Browsing the list of functions in dplR will give you an idea of what else is available. ?dplR "],["chronology-building.html", "Chapter 4 Chronology Building 4.1 Introduction 4.2 Data Sets 4.3 Traditional Chronology 4.4 Using a Sample Depth Cutoff 4.5 Using SSS as a Cutoff 4.6 Chronology Uncertainty 4.7 The ARSTAN Chronology 4.8 Stabalizing the Variance 4.9 Stripping out series by EPS 4.10 Conclusion", " Chapter 4 Chronology Building 4.1 Introduction The creation of a mean-value chronology is one of the central tasks for a dendrochronologist. It’s most commonly done by calculating the mean of each year across all the series in the data. However, there are several more ways to approach the problem and this document explains a few different ways of going about it. The use of the signal-free method of chronology building with ssf is complex enough to warrant its own chapter. 4.2 Data Sets Throughout this document we will use the onboard data set wa082 which gives the raw ring widths for Pacific silver fir Abies amabilis at Hurricane Ridge in Washington, USA. There are 23 series covering 286 years. library(dplR) ## This is dplR version 1.7.4. ## dplR is part of openDendro https://opendendro.org. ## New users can visit https://opendendro.github.io/dplR-workshop/ to get started. data(wa082) plot(wa082, plot.type=&quot;spag&quot;) By the way, if this is all new to you – go back and look at earlier chapters and proceed immediately to a good primer on dendrochronology like Fritts (2001). 4.3 Traditional Chronology Let us make a few chronologies from the wa082 data after detrending each series with an age-dependent spline (using ads). Detrending is an enormously complicated area unto itself and more deserving of a chapter than chronology building is. We over some of the issues around detrending in the prior chapter. There is a reason, after all, that dendrochronologists have been arguing about detrending for decades. wa082RWI &lt;- detrend(wa082, method=&quot;AgeDepSpline&quot;) The simplest way to make a chronology in dplR is chronology is with the chron function which also has a plot method. This defaults to building a mean-value chronology by averaging the rows of the rwi data using Tukey’s biweight robust mean (function tbrm). Here it is with a a 30-year smoothing spline for visualization. wa082Crn &lt;- chron(wa082RWI) str(wa082Crn) ## Classes &#39;crn&#39; and &#39;data.frame&#39;: 286 obs. of 2 variables: ## $ std : num 1.147 0.913 0.914 0.624 0.855 ... ## $ samp.depth: num 1 1 1 1 2 2 2 2 3 3 ... plot(wa082Crn, add.spline=TRUE, nyrs=30) Note the structure (str) of the output object states that it is class crn. This means that there are some generic functions that dplR has ready to work with this kind of object (e.g., time to extract the years, and plot which then calls crn.plot). The chron function will also compute a residual chronology by prewhitening the series before averaging. If the prewhiten flag is set to TRUE, each series is whitened using ar prior to averaging. The residual chronology is thus white noise. Note that the crn object below has two columns with chronologies as well as the sample depth in a third column. wa082CrnResid &lt;- chron(wa082RWI, prewhiten = TRUE) str(wa082CrnResid) ## Classes &#39;crn&#39; and &#39;data.frame&#39;: 286 obs. of 3 variables: ## $ std : num 1.147 0.913 0.914 0.624 0.855 ... ## $ res : num NA NA NA NA NA ... ## $ samp.depth: num 1 1 1 1 2 2 2 2 3 3 ... plot(wa082CrnResid) 4.4 Using a Sample Depth Cutoff A relatively simple addition to the traditional chronology is to truncate the chronology when the sample depth gets to a certain threshold. The output from the chron function contains a column called samp.depth which shows the number of series that are average for a particular year. We can use the subset function to modify the chronology. A standard method is to truncate the sample depth to a minimum of five series. head(wa082Crn) ## std samp.depth ## 1698 1.1472 1 ## 1699 0.9133 1 ## 1700 0.9135 1 ## 1701 0.6244 1 ## 1702 0.8551 2 ## 1703 0.6795 2 wa082CrnTrunc &lt;- subset(wa082Crn, samp.depth &gt; 4) plot(wa082CrnTrunc,add.spline=T,nyrs=30) It would likely be more robust to recalculate the ring-width indices object by truncating the rwl file and then making a chronology which could be done by nesting commands via: wa082CrnTrunc &lt;- chron(detrend(wa082[wa082Crn$samp.depth &gt; 4,], method=&quot;AgeDepSpline&quot;)) The result in this case is likely to be virtually identical to truncating after calculating the chronology but still seems like a good practice. 4.5 Using SSS as a Cutoff A more interesting and likely more robust approach is to truncate via the subsample signal strength (SSS). Just for fun, I’ll show how we can do this using tidy syntax and ggplot. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.6 ✓ dplyr 1.0.8 ## ✓ tidyr 1.2.0 ✓ stringr 1.4.0 ## ✓ readr 2.1.2 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() wa082Ids &lt;- autoread.ids(wa082) sssThresh &lt;- 0.85 wa082SSS &lt;- sss(wa082RWI, wa082Ids) yrs &lt;- time(wa082) yrCutoff &lt;- max(yrs[wa082SSS &lt; sssThresh]) ggplot() + geom_rect(aes(ymin=-Inf, ymax=Inf,xmin=-Inf,xmax=yrCutoff), fill=&quot;darkred&quot;,alpha=0.5) + annotate(geom = &quot;text&quot;,y=1.5,x=1725,label=&quot;SSS &lt; 0.85&quot;)+ geom_hline(yintercept = 1,linetype=&quot;dashed&quot;) + geom_line(aes(x=yrs,y=wa082Crn$std)) + labs(x=&quot;Year&quot;,y=&quot;RWI&quot;) + theme_minimal() Now we can cutoff the rwl data and redo the chronology. wa082RwlSSS &lt;- wa082[wa082SSS &gt; sssThresh,] wa082RwiSSS &lt;- detrend(wa082RwlSSS, method=&quot;AgeDepSpl&quot;) wa082CrnSSS &lt;- chron(wa082RwiSSS) ggplot() + geom_hline(yintercept = 1,linetype=&quot;dashed&quot;) + geom_line(aes(x=time(wa082CrnSSS),y=wa082CrnSSS$std)) + geom_line(aes(x=time(wa082CrnSSS), y=caps(wa082CrnSSS$std,nyrs = 30)), color=&quot;darkred&quot;) + labs(x=&quot;Year&quot;,y=&quot;RWI&quot;) + theme_minimal() 4.6 Chronology Uncertainty Typically we calculate a chronology by taking the average of each year from the ring-width indices. And that is typically the biweight robust mean. The function chron like many of the functions in dplR are relatively simple chunks of code that are used for convenience. We can make our own chronology and get the mean plus two standard errors of the yearly growth quite simply. It’s important for new users of dplR not to get stuck with just the available functions and to roll your own code. wa082AvgCrn &lt;- apply(wa082RwiSSS,1,mean,na.rm=TRUE) se &lt;- function(x){ x2 &lt;- na.omit(x) n &lt;- length(x2) sd(x2)/sqrt(n) } wa082AvgCrnSE &lt;- apply(wa082RwiSSS,1,se) dat &lt;- data.frame(yrs =as.numeric(names(wa082AvgCrn)), std = wa082AvgCrn, lwr = wa082AvgCrn - wa082AvgCrnSE*2, upr = wa082AvgCrn + wa082AvgCrnSE*2) ggplot(dat,aes(x=yrs)) + geom_hline(yintercept = 1,linetype=&quot;dashed&quot;) + geom_ribbon(aes(ymin=lwr,ymax=upr), alpha=0.5,fill=&quot;blue&quot;) + geom_line(aes(y=std),col=&quot;grey30&quot;) + labs(x=&quot;Year&quot;,y=&quot;RWI&quot;) + theme_minimal() It is interesting to note how the uncertainty increases towards the end of the chronology. This is somewhat unusual. 4.6.1 Bootstrapping confidence intervals The above method uses a parametric approach to quantifying uncertainty. We can also use the boot library which is an incredibly powerful suite of functions for using resampling techniques to calculate almost any imaginable statistic. Although boot and is used ubiquitously throughout R, its syntax is byzantine. Because of that we have created a wrapper for the boot.ci function will generate a mean-value chronology with bootstrapped confidence intervals. Here is a chronology using the wa082RwiSSS again but with 99% confidence intervals around the robust mean generated with 500 boostrap replicates. wa082CrnCI &lt;- chron.ci(wa082RwiSSS, biweight = TRUE, R = 500, conf = 0.99) head(wa082CrnCI) ## yrs std lowerCI upperCI samp.depth ## 1764 1764 0.9974 0.8416 1.175 9 ## 1765 1765 1.1763 0.9827 1.387 9 ## 1766 1766 1.0711 0.9266 1.217 9 ## 1767 1767 1.0074 0.7170 1.284 9 ## 1768 1768 1.1974 0.9280 1.558 9 ## 1769 1769 1.2375 1.0287 1.445 9 ggplot(wa082CrnCI,aes(x=yrs)) + geom_hline(yintercept = 1,linetype=&quot;dashed&quot;) + geom_ribbon(aes(ymin=lowerCI,ymax=upperCI), alpha=0.5,fill=&quot;blue&quot;) + geom_line(aes(y=std),col=&quot;grey30&quot;) + labs(x=&quot;Year&quot;,y=&quot;RWI&quot;) + theme_minimal() 4.7 The ARSTAN Chronology The function chron.ars produces the so-called ARSTAN chronology which retains the autoregressive structure of the input data. Users unfamiliar with the concept should dive into Cook (1985). This produces three mean-value chronologies: standard, residual, and ARSTAN. The standard chronology is the (biweight) mean value across rows and identical to chron. The residual chronology is the prewhitened chronology as described by Cook (1985) and uses uses multivariate autoregressive modeling to determine the order of the AR process. It’s important to note that residual chronology produced here is different than the simple residual chronology produced by chron which returns the residuals of an AR process using a naive call to ar. But in practice the results will be similar. For more on the residual chronology in this function, see pp. 153-154 in Cook (1985). The ARSTAN chronology builds on the residual chronology but returns a re-whitened chronology where the pooled AR coefficients from the multivariate autoregressive modeling are reintroduced. wa082ArsCrnSSS &lt;- chron.ars(wa082RwiSSS) ## Pooled AR Summary ## ACF ## ar(0) ar(1) ar(2) ar(3) ar(4) ar(5) ar(6) ar(7) ar(8) ar(9) ar(10) ## 1.0000 0.3548 0.3356 0.3073 0.2524 0.2306 0.2201 0.1940 0.2093 0.2107 0.2063 ## AR Coefs ## 1 2 3 4 5 6 7 8 ## ar(1) -0.3548 0.0000 0.0000 0.00000 0.00000 0.00000 0.000000 0.00000 ## ar(2) -0.2697 -0.2399 0.0000 0.00000 0.00000 0.00000 0.000000 0.00000 ## ar(3) -0.2314 -0.1968 -0.1598 0.00000 0.00000 0.00000 0.000000 0.00000 ## ar(4) -0.2197 -0.1825 -0.1429 -0.07294 0.00000 0.00000 0.000000 0.00000 ## ar(5) -0.2156 -0.1744 -0.1326 -0.06051 -0.05657 0.00000 0.000000 0.00000 ## ar(6) -0.2124 -0.1710 -0.1251 -0.05060 -0.04432 -0.05685 0.000000 0.00000 ## ar(7) -0.2105 -0.1695 -0.1234 -0.04655 -0.03878 -0.04997 -0.032391 0.00000 ## ar(8) -0.2084 -0.1663 -0.1209 -0.04357 -0.03087 -0.03911 -0.018905 -0.06406 ## ar(9) -0.2044 -0.1651 -0.1185 -0.04164 -0.02815 -0.03156 -0.008518 -0.05105 ## ar(10) -0.2013 -0.1625 -0.1181 -0.04003 -0.02672 -0.02943 -0.002472 -0.04262 ## 9 10 ## ar(1) 0.00000 0.00000 ## ar(2) 0.00000 0.00000 ## ar(3) 0.00000 0.00000 ## ar(4) 0.00000 0.00000 ## ar(5) 0.00000 0.00000 ## ar(6) 0.00000 0.00000 ## ar(7) 0.00000 0.00000 ## ar(8) 0.00000 0.00000 ## ar(9) -0.06245 0.00000 ## ar(10) -0.05202 -0.05102 ## AIC ## ar(0) ar(1) ar(2) ar(3) ar(4) ar(5) ar(6) ar(7) ar(8) ar(9) ar(10) ## 1775 1748 1737 1733 1734 1735 1736 1738 1739 1740 1742 ## Selected Order ## [1] 3 str(wa082ArsCrnSSS) ## Classes &#39;crn&#39; and &#39;data.frame&#39;: 220 obs. of 4 variables: ## $ std : num 0.997 1.176 1.071 1.007 1.197 ... ## $ res : num NaN NaN NaN NA NA ... ## $ ars : num NaN NaN NaN 1.04 1.23 ... ## $ samp.depth: num 9 9 9 9 9 9 10 10 11 12 ... wa082ArsCrnSSS &lt;- wa082ArsCrnSSS[,3:4] plot(wa082ArsCrnSSS,add.spline=TRUE,nyrs=20) 4.8 Stabalizing the Variance The function chron.stabilized builds a variance stabilized mean-value chronology following Frank et al. (2006). The code for this function was written by David Frank and adapted for dplR by Stefan Klesse. The stabilized chronology accounts for both temporal changes in the interseries correlation and sample depth to produce a mean value chronology with stabilized variance. wa082StabCrnSSS &lt;- chron.stabilized(wa082RwiSSS,winLength=101) str(wa082StabCrnSSS) # not class crn ## &#39;data.frame&#39;: 220 obs. of 2 variables: ## $ adj.crn : num 1.02 1.14 1.07 1.02 1.16 ... ## $ samp.depth: num 9 9 9 9 9 9 10 10 11 12 ... dat &lt;- data.frame(yrs = as.numeric(rownames(wa082StabCrnSSS)), Chron = wa082StabCrnSSS$adj.crn) %&gt;% mutate(Smoothed = caps(Chron,nyrs=20)) %&gt;% pivot_longer(cols = -yrs,names_to = &quot;variable&quot;, values_to = &quot;msmt&quot;) ggplot(dat,aes(x=yrs,y=msmt,color=variable)) + geom_hline(yintercept = 1,linetype=&quot;dashed&quot;) + geom_line() + scale_color_manual(values = c(&quot;grey&quot;,&quot;darkred&quot;)) + labs(x=&quot;Year&quot;,y=&quot;RWI&quot;,title=&quot;Variable Stabilized Chronology&quot;) + theme_minimal() + theme(legend.title = element_blank(),legend.position = &quot;bottom&quot;) 4.9 Stripping out series by EPS We want to introduce one other approach that doesn’t deal explicitly with chronology building but can be used to build a better chronology. The strip.rwl function uses EPS-based chronology stripping where each series is assessed to see if its inclusion in the chronology improves the EPS (Fowler &amp; Boswijk, 2003). If it does not the series is dropped from the rwl object. As we will see in this example one series are excluded which causes a modest improvement in EPS. This function was contributed by Christain Zang. wa082StripRwl &lt;- strip.rwl(wa082, ids = wa082Ids) ## REMOVE -- Iteration 1: leaving series 712071 out. ## EPS improved from 0.907 to 0.908. ## ## REMOVE -- Iteration 2: no improvement of EPS. Aborting... ## REINSERT -- Iteration 1: no improvement of EPS. Aborting... wa082StripRwi &lt;- detrend(wa082StripRwl, method=&quot;Spline&quot;) wa082StripCrn &lt;- chron(wa082StripRwi) wa082StripCrn &lt;- subset(wa082StripCrn, samp.depth &gt; 4) plot(wa082StripCrn, add.spline=TRUE, nyrs=30) 4.10 Conclusion We have tried to introduce a few ways of building chronologies with dplR that are either typical (like truncating by sample depth) or less commonly used. In this chapter we aren’t advocating any particular method but trying to get the users familiar with ways of interacting with the objects that dplR produces. Once the user understands the data structures the rest of R opens up. Again, we feel that it is important to reiterate that the advantage of using dplR is that it gets the analyst to use R and thus have access to the essentially limitless tool that it provides. Go foRth! "],["simple-signal-free.html", "Chapter 5 Simple Signal-Free 5.1 Introduction 5.2 Packages 5.3 Data 5.4 Making the Simple Signal-Free Chronology 5.5 Changing the Detrending Method 5.6 Walk Through 5.7 Final Thoughts", " Chapter 5 Simple Signal-Free 5.1 Introduction This is an initial draft that demonstrates how to build a simple signal-free chronology using the ssf function. Although this function is used to build chronologies, it is sufficiently different in its philosophy that it warrants its own chapter. In most regards the ssf function follows the procedures laid out by Melvin and Briffa’s classic 2008 paper in Dendrochronologia (Melvin &amp; Briffa, 2008). This is not a treatise on the pros and cons of the signal-free approach but attempts to demonstrate how the ssf function in can be applied to ring-width data. This particular implementation of the signal-free approach is quite limited in its scope and users who want more control and options (such as RCS) should look to the CRUST program detailed in Melvin and Briffa Melvin &amp; Briffa (2014b) available on GitHub. 5.2 Packages Here are the packages we will use. library(dplR) library(tidyverse) library(cowplot) library(ggExtra) library(PNWColors) 5.3 Data We will demonstrate ssf with Bristlecone pine ring widths from Campito Mountain which is included with dplR. data(ca533) class(ca533) ## [1] &quot;rwl&quot; &quot;data.frame&quot; dat &lt;- ca533 Note the class of the object which is rwl. This means, simply, that the data are in a format that dplR understands. It has series in columns and years stored as rownames. Objects of class rwl can be summarized, plotted, used for detrending and so on. Before we continue we will truncate the the sample depth to a minimum of five series. There is nothing magic about the sample depth but having five series is usually sufficient for having a robust chronology. sampDepth &lt;- rowSums(!is.na(dat)) datTrunc &lt;- dat[sampDepth &gt; 4,] We will now grab a few variables that will come in handy during this vigentte although they aren’t needed for a simple run of ssf. yrs &lt;- time(datTrunc) medianSegLength &lt;- floor(median(rwl.stats(datTrunc)$year)) sampDepth &lt;- rowSums(!is.na(datTrunc)) normalizedSampleDepth &lt;- sqrt(sampDepth-1)/sqrt(max(sampDepth-1)) We will also set up some color palettes to use in plotting. # get some color palettes from Jake L&#39;s PNWColors seriesColors &lt;- pnw_palette(name=&quot;Starfish&quot;,n=dim(datTrunc)[2]) divColors &lt;- pnw_palette(&quot;Moth&quot;,5) Throughout this document, we will make use of both tidyverse syntax and do much of the plotting in ggplot. E.g., here is a plot of all the ring widths that we will use. rawRW &lt;- data.frame(yrs = yrs, datTrunc) %&gt;% pivot_longer(!yrs,names_to = &quot;series&quot;, values_to = &quot;msmt&quot;) ggplot(data=rawRW,mapping = aes(x=yrs,y=msmt,color=series)) + geom_line(alpha=0.5) + scale_color_manual(values = seriesColors) + labs(y=&quot;mm&quot;,x=&quot;Years&quot;,caption = &quot;Raw Measurements&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;) ## Warning: Removed 11868 row(s) containing missing values (geom_path). NB ggplot is issuing a warning above telling us that there are quite a few NA values in the data. That is nothing to worry about – it is merely saying that the series have different start and end dates. 5.4 Making the Simple Signal-Free Chronology Now we create the simple signal-free chronology. In its most direct use, the ssf function outputs a crn object that has a simple onboard plotting method (crn.plot) which we will show rather than make a ggplot of the same data. Don’t worry, there is plenty of fancy plotting below. ssfCrn &lt;- ssf(rwl = datTrunc) ## Data read. First iteration done. ## Iteration: 2 Median Abs Diff: 0.00415 (12.05% of threshold) ## Iteration: 3 Median Abs Diff: 0.00217 (23.08% of threshold) ## Iteration: 4 Median Abs Diff: 0.00291 (17.19% of threshold) ## Iteration: 5 Median Abs Diff: 0.00133 (37.68% of threshold) ## Iteration: 6 Median Abs Diff: 0.001 (50.02% of threshold) ## Iteration: 7 Median Abs Diff: 0.00076 (66.13% of threshold) ## Iteration: 8 Median Abs Diff: 0.00066 (75.66% of threshold) ## Iteration: 9 Median Abs Diff: 0.00055 (91.4% of threshold) ## Iteration: 10 Median Abs Diff: 0.00047 (105.5% of threshold) ## Simple Signal Free Chronology Complete ## ssf was called with these arguments ## Detrending method: AgeDepSpline ## nyrs: ## pos.slope: FALSE ## maxIterations: 25 ## madThreshold: 5e-04 str(ssfCrn) # note class of output object ## Classes &#39;crn&#39; and &#39;data.frame&#39;: 1007 obs. of 2 variables: ## $ sfc : num 1.219 1.487 0.836 0.664 0.57 ... ## $ samp.depth: num 5 5 5 5 5 5 5 5 5 5 ... plot(ssfCrn,add.spline=TRUE,nyrs=50, crn.line.col=divColors[3],spline.line.col=divColors[1], crn.lwd=1.5,spline.lwd=2) In the above, the signal-free chronology is shown with a 50-year smoothing spline added for visualization of low-frequency variability. The algorithm converged after 10 iterations. 5.5 Changing the Detrending Method The help file for ssf gives more information on possible arguments. See ?ssf for details. By default the function uses an age-depended spline (dplR function ads) for detrending. But we can also use a cubic smoothing spline (caps). We will arbitrarily set the stiffness to the median segment length of the input data, which is 627. Whether this is a wise choice or not is debatable. ssfCrn2 &lt;- ssf(rwl = datTrunc,method=&quot;Spline&quot;,nyrs=medianSegLength) ## Data read. First iteration done. ## Iteration: 2 Median Abs Diff: 0.00212 (23.57% of threshold) ## Iteration: 3 Median Abs Diff: 0.00185 (26.97% of threshold) ## Iteration: 4 Median Abs Diff: 0.0017 (29.36% of threshold) ## Iteration: 5 Median Abs Diff: 0.00153 (32.63% of threshold) ## Iteration: 6 Median Abs Diff: 0.00144 (34.65% of threshold) ## Iteration: 7 Median Abs Diff: 0.00138 (36.32% of threshold) ## Iteration: 8 Median Abs Diff: 0.00125 (39.92% of threshold) ## Iteration: 9 Median Abs Diff: 0.00121 (41.25% of threshold) ## Iteration: 10 Median Abs Diff: 0.00114 (43.97% of threshold) ## Iteration: 11 Median Abs Diff: 0.00106 (47.17% of threshold) ## Iteration: 12 Median Abs Diff: 0.00103 (48.5% of threshold) ## Iteration: 13 Median Abs Diff: 0.00098 (50.96% of threshold) ## Iteration: 14 Median Abs Diff: 0.00095 (52.75% of threshold) ## Iteration: 15 Median Abs Diff: 0.00089 (55.93% of threshold) ## Iteration: 16 Median Abs Diff: 0.00086 (58.06% of threshold) ## Iteration: 17 Median Abs Diff: 0.00081 (61.74% of threshold) ## Iteration: 18 Median Abs Diff: 0.00074 (67.72% of threshold) ## Iteration: 19 Median Abs Diff: 0.00067 (74.48% of threshold) ## Iteration: 20 Median Abs Diff: 0.00064 (77.98% of threshold) ## Iteration: 21 Median Abs Diff: 6e-04 (82.93% of threshold) ## Iteration: 22 Median Abs Diff: 0.00057 (88.15% of threshold) ## Iteration: 23 Median Abs Diff: 0.00053 (94.86% of threshold) ## Iteration: 24 Median Abs Diff: 0.00048 (104% of threshold) ## Simple Signal Free Chronology Complete ## ssf was called with these arguments ## Detrending method: Spline ## nyrs: 627 ## maxIterations: 25 ## madThreshold: 5e-04 plot(ssfCrn2,add.spline=TRUE,nyrs=50, crn.line.col=divColors[3],spline.line.col=divColors[1], crn.lwd=1.5,spline.lwd=2) Note that the algorithm takes more iterations to converge but produces a qualitatively similar chronology. 5.6 Walk Through To demonstrate how the signal free process works we can redo the chronology, this time returning information on the process at each iteration. Note the change in the structure (str) of the output object. ssfCrn &lt;- ssf(rwl = datTrunc,return.info = TRUE,verbose = FALSE) str(ssfCrn) ## List of 11 ## $ infoList :List of 5 ## ..$ method : chr &quot;AgeDepSpline&quot; ## ..$ nyrs : NULL ## ..$ pos.slope : logi FALSE ## ..$ maxIterations: num 25 ## ..$ madThreshold : num 5e-04 ## $ iter0Crn :Classes &#39;crn&#39; and &#39;data.frame&#39;: 1007 obs. of 2 variables: ## ..$ std : num [1:1007] 1.288 1.557 0.884 0.703 0.6 ... ## ..$ samp.depth: num [1:1007] 5 5 5 5 5 5 5 5 5 5 ... ## $ ssfCrn :Classes &#39;crn&#39; and &#39;data.frame&#39;: 1007 obs. of 2 variables: ## ..$ sfc : num [1:1007] 1.219 1.487 0.836 0.664 0.57 ... ## ..$ samp.depth: num [1:1007] 5 5 5 5 5 5 5 5 5 5 ... ## $ sfRW_Array : num [1:1007, 1:34, 1:10] NA NA NA NA NA NA NA NA NA NA ... ## $ sfRWRescaled_Array : num [1:1007, 1:34, 1:10] NA NA NA NA NA NA NA NA NA NA ... ## $ sfRWRescaledCurves_Array: num [1:1007, 1:34, 1:10] NA NA NA NA NA NA NA NA NA NA ... ## $ sfRWI_Array : num [1:1007, 1:34, 1:10] NA NA NA NA NA NA NA NA NA NA ... ## $ sfCrn_Mat : num [1:1007, 1:10] 1.264 1.394 0.849 0.666 0.568 ... ## $ hfCrn_Mat : num [1:1007, 1:25] 0.313 0.443 -0.102 -0.285 -0.383 ... ## $ hfCrnResids_Mat : num [1:1007, 1:9] -0.02912 0.07009 -0.02075 -0.00941 -0.02138 ... ## $ MAD_Vec : num [1:9] 0.00415 0.00217 0.00291 0.00133 0.001 ... The ssfCrn object is now a list with information on all the iterations that algorithm has run through. We can save these to their own objects for easier access. sfRW_Array &lt;- ssfCrn$sfRW_Array sfRWRescaled_Array &lt;- ssfCrn$sfRWRescaled_Array sfRWRescaledCurves_Array &lt;- ssfCrn$sfRWRescaledCurves_Array sfRWI_Array &lt;- ssfCrn$sfRWI_Array sfCrn_Mat &lt;- ssfCrn$sfCrn_Mat hfCrn_Mat &lt;- ssfCrn$hfCrn_Mat hfCrnResids_Mat &lt;- ssfCrn$hfCrnResids_Mat MAD_Vec &lt;- ssfCrn$MAD_Vec 5.6.1 Step 1 (Iteration 0) This is the initial, naive chronology at iteration zero. plot(ssfCrn$iter0Crn,add.spline=TRUE,nyrs=50, crn.line.col=divColors[4],spline.line.col=divColors[2], crn.lwd=1.5,spline.lwd=2) We use this as a starting point and begin the iterations. 5.6.2 Steps 2 and 3 (Iteration 1) The algorithm first creates signal-free measurements for each series. At the first iteration these are the measurements divided by the initial chronology. These signal-free measurements are output in sfRW_Array which is an array of years by series, by iteration. The signal-free measurements are then rescaled to have the original mean of each series. sfRWRescaled &lt;- data.frame(yrs = yrs, sfRWRescaled_Array[,,1]) %&gt;% pivot_longer(!yrs,names_to = &quot;series&quot;, values_to = &quot;msmt&quot;) ggplot(data=sfRWRescaled,mapping = aes(x=yrs,y=msmt,color=series)) + geom_line(alpha=0.75) + scale_color_manual(values = seriesColors) + labs(caption=&quot;Rescaled Signal Free Measurements at Iteration 1&quot;, y=&quot;mm&quot;, x=&quot;Years&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;) ## Warning: Removed 11868 row(s) containing missing values (geom_path). 5.6.3 Steps 4 and 5 (Iteration 1) In these steps the algorithm first looks for any places in the rescaled ring width array that have a sample depth of one and replaces the individual signal-free measurements values with the original measurement values. Then the curve fitting is repeated giving the detrending curves for iteration one. sfRWRescaledCurves &lt;- data.frame(yrs = yrs, sfRWRescaledCurves_Array[,,1]) %&gt;% pivot_longer(!yrs,names_to = &quot;series&quot;, values_to = &quot;msmt&quot;) ggplot(data=sfRWRescaledCurves,mapping = aes(x=yrs,y=msmt,color=series)) + geom_line(alpha=0.75,size=1) + scale_color_manual(values = seriesColors) + labs(caption=&quot;Detrending Curves at Iteration 1&quot;, y=&quot;mm&quot;,x=&quot;Years&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;) ## Warning: Removed 11868 row(s) containing missing values (geom_path). 5.6.4 Step 6 and 7 (Iteration 1) With the detrending curves above, we make the first signal-free ring-width indices by dividing the original measurements by the curves above. sfRWI &lt;- data.frame(yrs = yrs, sfRWI_Array[,,1]) %&gt;% pivot_longer(!yrs,names_to = &quot;series&quot;, values_to = &quot;msmt&quot;) ggplot(data=sfRWI,mapping = aes(x=yrs,y=msmt,color=series)) + geom_line(alpha=0.5) + geom_hline(yintercept = 1,linetype=&quot;dashed&quot;) + scale_color_manual(values = seriesColors) + labs(caption=&quot;Signal Free Indices at Iteration 1&quot;, y=&quot;RWI&quot;, x=&quot;Years&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;) ## Warning: Removed 11868 row(s) containing missing values (geom_path). With these ring width indices we can make the signal-free chronology for the first iteration which we will plot with a 50-year smoothing spline. sfCrn &lt;- data.frame(yrs = yrs, msmt = sfCrn_Mat[,1], msmstSm= caps(sfCrn_Mat[,1],nyrs = 50)) %&gt;% pivot_longer(!yrs,names_to = &quot;series&quot;, values_to = &quot;msmt&quot;) ggplot(data=sfCrn,mapping = aes(x=yrs,y=msmt, color=series, size=series, alpha=series)) + geom_hline(yintercept = 1,linetype=&quot;dashed&quot;) + geom_line() + scale_color_manual(values = divColors[c(3,3)]) + scale_size_manual(values = c(1,0.5)) + scale_alpha_manual(values = c(1,0.5)) + labs(caption=&quot;Signal Free Chronology at Iteration 1&quot;, y=&quot;RWI&quot;,x=&quot;Years&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;) 5.6.5 Step 8 (Begin Iterations) Now we begin iterating through the steps above until the stopping criteria is met or until the maximum number of iterations is reached. Here we show how the chronology changes from iteration one to iteration two and how the high-pass filtering is used to calculate the median absolute difference (MAD) used as the stopping criteria. sfCrn &lt;- data.frame(yrs = yrs, msmt = sfCrn_Mat[,1:2]) %&gt;% rename(`Iteration 1` = 2,`Iteration 2` = 3) %&gt;% pivot_longer(!yrs,names_to = &quot;Iteration&quot;, values_to = &quot;msmt&quot;) p1 &lt;- ggplot(data=sfCrn, mapping = aes(x=yrs,y=msmt,color=Iteration, linetype=Iteration, alpha=Iteration)) + geom_hline(yintercept = 1,linetype=&quot;dashed&quot;) + geom_line() + scale_color_manual(values = divColors[c(4,2)]) + scale_alpha_manual(values = c(0.75,0.75)) + labs(caption =&quot;Signal Free Chronology at Iteration 1 and 2&quot;, y=&quot;RWI&quot;,x=&quot;Years&quot;) + lims(y=c(0,3)) + theme_cowplot() + guides(colour = guide_legend(nrow = 1)) + theme(legend.position=c(.1,.1),legend.title = element_blank()) sfCrnCompare &lt;- data.frame(sfCrn_Mat[,1:2]) p2 &lt;- ggplot(data=sfCrnCompare, mapping = aes(x=X1,y=X2)) + geom_point(alpha=0.5,color=divColors[3]) + geom_abline(slope=1,intercept = 0, linetype=&quot;dashed&quot;) + labs(y=element_blank(), x= element_blank()) + coord_equal() + theme_cowplot() + theme(axis.text.x = element_blank(), axis.text.y = element_blank()) p2 &lt;- ggMarginal(p2, type=&quot;boxplot&quot;, size=6, xparams = list(fill = divColors[4], color = &quot;grey30&quot;), yparams = list(fill = divColors[2], color = &quot;grey30&quot;)) ggdraw(p1) + draw_plot(plot = p2, x = .05, y = .5, width = .5, height = .5) These chronologies are quite similar but differ at higher RWI values. The median absolute error between these two chronologies is calculated on the high frequency component which is calculated using a cubic smoothing spline with stiffness set to the median segment length. highFreqCrn &lt;- data.frame(yrs = yrs, highFreqIter1 = hfCrn_Mat[,1], highFreqIter2 = hfCrn_Mat[,2]) %&gt;% rename(`Iteration 1` = 2,`Iteration 2` = 3) %&gt;% pivot_longer(!yrs,names_to = &quot;Iteration&quot;, values_to = &quot;msmt&quot;) # plot the high freq chrons p1 &lt;- ggplot(data=highFreqCrn, mapping = aes(x=yrs,y=msmt,color=Iteration, linetype=Iteration, alpha=Iteration)) + geom_hline(yintercept = 0,linetype=&quot;dashed&quot;) + geom_line() + scale_color_manual(values = divColors[c(4,2)]) + scale_alpha_manual(values = c(0.75,0.75)) + labs(caption=&quot;High Freq Chronology at Iteration 1 and 2&quot;, y=&quot;RWI&quot;,x=&quot;Years&quot;) + theme_cowplot() + theme(legend.position = &quot;top&quot;,legend.title = element_blank()) # plot the high freq chron difference (stored in hfCrnResids_Mat) highFreqDiff &lt;- data.frame(yrs=yrs, difference = hfCrnResids_Mat[,1]) p2 &lt;- ggplot(highFreqDiff,aes(x=yrs,y=difference)) + geom_hline(yintercept = 0,linetype=&quot;dashed&quot;) + geom_line(color=divColors[3]) + labs(y=&quot;RWI Difference&quot;,x=&quot;Years&quot;, caption=paste0(&quot;Iteration 1v2 MAD &quot;, round(MAD_Vec[1],4))) + lims(y=c(-0.05,0.1)) + theme_cowplot() + theme(legend.position = &quot;none&quot;, legend.title = element_blank()) plot_grid(p1, p2,nrow = 2) In the lower plot it’s clear that the difference in the chronologies is highest at the start and end. We calculate the median absolute difference using the normalized sample depth as: median(abs(hfCrn_Mat[,2]*normalizedSampleDepth - hfCrn_Mat[,1]*normalizedSampleDepth)) ## [1] 0.004149 This is also returned in MAD_Vec. Since this value is greater than the the stopping criteria of 5e-04, the algorithm continues to iterate until the difference is below the threshold or the maximum number of iterations is reached. Here is the difference plot between iterations nine and ten. highFreqDiff &lt;- data.frame(yrs=yrs, difference = hfCrnResids_Mat[,9]) ggplot(highFreqDiff,aes(x=yrs,y=difference)) + geom_hline(yintercept = 0,linetype=&quot;dashed&quot;) + geom_line(color=divColors[3]) + labs(y=&quot;RWI Difference&quot;,x=&quot;Years&quot;, caption=paste0(&quot;Iteration 9v10 MAD &quot;, round(MAD_Vec[9],4))) + lims(y=c(-0.05,0.1)) + theme_cowplot() + theme(legend.position = &quot;none&quot;, legend.title = element_blank()) 5.6.6 Iterations Visualized We can see the progression of the output chronology relatively clearly by plotting the 50-year smoothing spline of the final chronology for each iteration. crnCols &lt;- pnw_palette(name=&quot;Starfish&quot;,n=dim(sfCrn_Mat)[2]) # smooth chrons sfCrnSm &lt;- data.frame(yrs = yrs, msmt = apply(sfCrn_Mat,2,caps,nyrs=50)) %&gt;% rename_with(.fn = seq, .cols = -1) %&gt;% pivot_longer(!yrs,names_to = &quot;Iteration&quot;, values_to = &quot;msmt&quot;) %&gt;% mutate(Iteration = as.numeric(Iteration)) p1 &lt;- ggplot() + geom_hline(yintercept = 1,linetype=&quot;dashed&quot;) + geom_line(data=sfCrnSm, mapping = aes(x=yrs,y=msmt,color=factor(Iteration)), alpha=0.75,size=1) + scale_color_manual(values = crnCols) + labs(caption=&quot;Signal Free Chronology&quot;, x=&quot;Years&quot;,y=&quot;RWI&quot;) + theme_cowplot() + theme(legend.position=&quot;none&quot;) MADdf &lt;- data.frame(Iteration = 2:dim(sfCrn_Mat)[2], MAD = MAD_Vec) p2 &lt;- ggplot(MADdf, aes(x = Iteration,y = MAD,color=factor(Iteration))) + geom_hline(yintercept = 5e-04,color=&quot;grey30&quot;,linetype=&quot;dotted&quot;) + geom_line(color=&quot;grey30&quot;) + geom_point(size=2,alpha=0.9) + scale_color_manual(values = crnCols) + scale_x_continuous(breaks=seq(2,max(MADdf$Iteration),by=2), limits = c(1,max(MADdf$Iteration)+1), expand = c(0,0)) + scale_y_continuous(limits = c(0,max(MADdf$MAD))) + theme_cowplot() + theme(legend.position = &quot;none&quot;, plot.background = element_rect(fill = &quot;gray95&quot;)) ggdraw(p1) + draw_plot(plot = p2, x = .15, y = .6, width = .4, height = .4) We can also use the gganimate package to combine some of these elements in a way that allows us to see the progression of the chronology through the fitting process. library(gganimate) library(magick) if(!file.exists(&quot;sfAnim.gif&quot;)){ # raw data sfCrn &lt;- data.frame(yrs = yrs, msmt = sfCrn_Mat) %&gt;% rename_with(.fn = seq, .cols = -1) %&gt;% pivot_longer(!yrs,names_to = &quot;Iteration&quot;, values_to = &quot;msmt&quot;) %&gt;% mutate(Iteration = as.numeric(Iteration)) %&gt;% mutate(Iteration2 = str_pad(as.character(Iteration),width=2,pad=&quot;0&quot;)) # smooth data sfCrnSm &lt;- data.frame(yrs = yrs, msmtsm = apply(sfCrn_Mat,2,caps,nyrs=50)) %&gt;% rename_with(.fn = seq, .cols = -1) %&gt;% pivot_longer(!yrs,names_to = &quot;Iteration&quot;, values_to = &quot;msmt&quot;) %&gt;% mutate(Iteration = as.numeric(Iteration)) %&gt;% mutate(Iteration2 = str_pad(as.character(Iteration),width=2,pad=&quot;0&quot;)) # MAD data MADdf &lt;- data.frame(Iteration = 1:dim(sfCrn_Mat)[2], IterationStatic = 1:dim(sfCrn_Mat)[2], MAD = c(NA,MAD_Vec)) %&gt;% mutate(Iteration2 = str_pad(as.character(Iteration),width=2,pad=&quot;0&quot;)) # hf diff data # diff data have to add a column of zeros for iter 1 highFreqDiff &lt;- data.frame(yrs=yrs, difference = cbind(0,hfCrnResids_Mat)) %&gt;% rename_with(.fn = seq, .cols = -1) %&gt;% pivot_longer(!yrs,names_to = &quot;Iteration&quot;, values_to = &quot;difference&quot;) %&gt;% mutate(Iteration = as.numeric(Iteration)) %&gt;% mutate(Iteration2 = str_pad(as.character(Iteration),width=2,pad=&quot;0&quot;)) # crn plot p1 &lt;- ggplot() + # layer 1 geom_hline(yintercept = 1,linetype=&quot;dashed&quot;) + # layer 2 geom_line(data=sfCrn,mapping = aes(x=yrs,y=msmt,color=Iteration2), alpha=0.25) + # layer 3 geom_line(data=sfCrnSm,mapping = aes(x=yrs,y=msmt,color=Iteration2), alpha=0.75,size=1) + scale_color_manual(values = crnCols) + labs(x=&quot;Years&quot;, y=&quot;RWI&quot;, caption=&quot;Signal Free Chronology&quot;, subtitle = &quot;Iteration: {as.integer(frame_time)}&quot;) + theme_cowplot() + theme(legend.position=&quot;none&quot;) # mad plot p2 &lt;- ggplot(MADdf, aes(x = IterationStatic,y = MAD)) + #static geom_hline(yintercept = 5e-04,color=&quot;grey30&quot;,linetype=&quot;dotted&quot;) + geom_point(aes(color = factor(IterationStatic)),size=4,alpha=0.9) + scale_color_manual(values = crnCols) + # dynamic # Direction of segment reversed below, less distracting geom_line(color=&quot;grey30&quot;) + geom_segment(aes(x = 1, xend = Iteration, yend = MAD), linetype = 2, colour = &#39;gray50&#39;) + geom_point(aes(x=Iteration),size=3,color=&quot;grey30&quot;,alpha=0.5) + scale_x_continuous(breaks=seq(2,max(MADdf$Iteration),by=2), limits = c(1,max(MADdf$Iteration)+1), expand = c(0,0)) + scale_y_continuous(limits = c(0,max(MADdf$MAD))) + coord_cartesian(clip = &#39;off&#39;) + labs(x=&quot;Iteration&quot;, y = &quot;MAD&quot;, caption=&quot;&quot;, subtitle = &quot;&quot;) + theme_cowplot() + theme(legend.position = &quot;none&quot;) # dif plot p3 &lt;- ggplot(highFreqDiff,aes(x=yrs,y=difference)) + geom_hline(yintercept = 0,linetype=&quot;dashed&quot;) + geom_line(aes(color=factor(Iteration))) + scale_color_manual(values = crnCols) + labs(y=&quot;RWI Difference&quot;,x=&quot;Years&quot;, caption=&quot;&quot;) + lims(y=c(-0.05,0.1)) + theme_cowplot() + theme(legend.position = &quot;none&quot;, legend.title = element_blank()) ## do the animations and write the final gif a1 &lt;- p1 + transition_time(Iteration) + shadow_mark(exclude_layer = 2) + enter_fade() a1_gif &lt;- animate(a1, nframes = 100, duration = 10, width = 6, height = 3, units = &quot;in&quot;, res = 200, renderer = magick_renderer()) a2 &lt;- p2 + transition_reveal(Iteration) a2_gif &lt;- animate(a2, nframes = 100, duration = 10, width = 3, height = 3, units = &quot;in&quot;, res = 200, renderer = magick_renderer()) a3 &lt;- p3 + transition_time(Iteration) + enter_fade() + exit_fade() a3_gif &lt;- animate(a3, nframes = 100, duration = 10, width = 6, height = 3, units = &quot;in&quot;, res = 200, renderer = magick_renderer()) combined_gif &lt;- image_montage(c(a1_gif[1],a2_gif[1],a3_gif[1]), tile = &quot;2x2&quot;, geometry_size_percent(100,100)) for(i in 2:100){ # nframe tmp &lt;- image_montage(c(a1_gif[i],a2_gif[i],a3_gif[i]), tile = &quot;2x2&quot;, geometry_size_percent(100,100)) combined_gif &lt;- c(combined_gif,tmp) } image_write(image = combined_gif,format = &quot;gif&quot;,path = &quot;sfAnim.gif&quot;) } knitr::include_graphics(&quot;sfAnim.gif&quot;) One thing to note with these data is that the upturns and downturns at the ends of the chronology are persistent even though they shrink vastly in the later iterations. 5.7 Final Thoughts This document shows how the algorithm progresses through the iterations in order to calculate the simple signal-free chronology. We should caution here that while there are excellent theoretical reasons to use the signal-free approach, the method is not a panacea for all applications. Detrending, after all, is a dark art and it is always up to the user to carefully evaluate the data and build chronologies with expectation that the final product is heavily dependent on the site and on the methods used. "],["xdate.html", "Chapter 6 xDate 6.1 Introduction 6.2 Using the xDater app 6.3 Crossdating by hand in dplR 6.4 Ruining a Perfectly Good Data Set 6.5 Series Correlation by Segment 6.6 Individual Series Correlation 6.7 Using Cross Correlation 6.8 Visual Crossdating 6.9 Conclusion", " Chapter 6 xDate In this document we cover basic crossdating techniques in dplR by deliberately misdating one of the series in a well-dated set of ring widths and tracking down the dating error. As with any dating enterprise, statistical crossdating is merely a tool and users should always rely on the wood to accurately date tree-ring data. 6.1 Introduction This gives an introduction of some of the crossdating functions in dplR. This is essentially a rehashing of (Bunn, 2010). Please cite that paper if you use dplR for crossdating. There is more detailed information on all these functions in the help files. 6.2 Using the xDater app Below I’ll walk through using dplR’s crossdating functions on the command line. But I’ve also built a Shiny app for doing crossdating in an interactive graphical workflow that is one of the rare cases where an app and GUI are more useful than working in a script. This is definitely still in development and I plan on cleaning it up soon. It’s meant to mimic a lot of the look and feel of COFECHA since that’s what most folks are used to using. Here it is: andybunn.shinyapps.io/xDateR/ While the xDater app is cool, the ringdater app from David Reynolds, David Edge &amp; Bryan Black is also very cool. Check that out too. 6.3 Crossdating by hand in dplR Here is a walk through of using dplR for crossdating on the command line. 6.3.1 Load dplR We will be usinf dplR in here. Load it: library(dplR) ## This is dplR version 1.7.4. ## dplR is part of openDendro https://opendendro.org. ## New users can visit https://opendendro.github.io/dplR-workshop/ to get started. 6.4 Ruining a Perfectly Good Data Set Throughout this document we will use the on-board data set co021 which gives the raw ring widths for Douglas fir Pseudotsuga menziesii at Mesa Verde in Colorado, USA. There are 35 series spanning 788 years. We will rename the co021 object to dat because we are going to mess around with it and it seems like good practice to rename it. data(co021) dat &lt;- co021 dat.sum &lt;- summary(dat) mean(dat.sum$year) ## [1] 564.9 mean(dat.sum$stdev) ## [1] 0.3232 mean(dat.sum$median) ## [1] 0.3211 mean(dat.sum$ar1) ## [1] 0.6038 mean(interseries.cor(dat)[, 1]) ## [1] 0.8478 plot(dat, plot.type=&quot;spag&quot;) We can see that this is a beautifully sensitive collection with long segment lengths, high standard deviation (relative to ring widths), large first-order autocorrelation, and a high mean interseries correlation (\\(\\mathrm{r}\\approx 0.84\\)). To demonstrate how crossdating works in dplR, we will take this perfectly lovely data set and corrupt the dating of one of the series. By doing so we will be able to reenact one of the most common tasks of the dendrochronologist: tracking down a misdated core. Here we will take a random series and remove one of the years of growth. This simulates a missing ring in the series. We will pick a random year in the core to give us a bit of a challenge in finding it. set.seed(4576) i &lt;- sample(x=nrow(dat), size=1) j &lt;- sample(x=ncol(dat), size=1) tmp &lt;- dat[, j] tmp &lt;- c(NA, tmp[-i]) dat[, j] &lt;- tmp We have now deleted the \\(i^{th}\\) observation from the \\(j^{th}\\) core while making sure that dat still has the appropriate numbers of rows. By sticking the NA at the start of the series it is as if we missed a ring while measuring. 6.5 Series Correlation by Segment The primary function for looking the crossdating of a tree-ring data set in dplR is corr.rwl.seg. This function looks at the correlation between each tree-ring series and a master chronology built from all the other series in the rwl object (leave-one-out principle). These correlations are calculated on overlapping segments (e.g., 50-year segments would be overlapped by 25 years). By default, each of the series is filtered to remove low-frequency variation prior to the correlation analysis. The help file has abundant details. Here we will look at overlapping 60 year segments. A plot is produced by default with corr.rwl.seg. In the corr.rwl.seg plot, each segment of each series is shown and colored by its correlation with the master. Each series is represented by two courses of lines with the bottom course adhering to the bottom axis timeline and the top course matching the upper axis timeline. Segments are colored according to the strength of the correlation between that segment and the master chronology. Blue correlates well (p-values less or equal to the user-set critical value) while potential dating problems are indicated by the red segments (p-values greater than the user-set critical value). Green lines show segments that do not completely overlap the time period and thus have no correlations calculated. Our modified data set indicates one series with dating problems. rwl.60 &lt;- corr.rwl.seg(dat, seg.length=60, pcrit=0.01) In this figure, each 60-year segment of each series in the modified Mesa Verde data set is shown and colored by its correlation with the master. Our modified data set indicates one series with dating problems. 6.6 Individual Series Correlation The low correlation between series 643114 and the master indicates a dating problem. Now that we suspect a dating problem, let us take a closer look at this problem child. The figure above shows that series 643114 begins to lose correlation with the master at the end of the 19th century. seg.60 &lt;- corr.series.seg(rwl=dat, series=&quot;643114&quot;, seg.length=60) Correlations between series 643114 and the master chronology are shown with horizontal lines according (60-year segments lagged by 30 years). A centered running correlation with a length of 60 years complements the segment correlations. The critical level is shown with a dashed line. 6.7 Using Cross Correlation This figure strongly indicates that the dating in the series 643114 begins to deteriorate between 1850 and 1910. We can create a window (win) of years and subset dat to the window if years we want to look at. Then we can look more closely at this time period and compute a cross-correlation function to look at lagged correlations to see if we can spot the dating problem. win &lt;- 1800:1960 dat.yrs &lt;- time(dat) dat.win &lt;- subset(dat,dat.yrs %in% win) ccf.30 &lt;- ccf.series.rwl(rwl=dat.win, series=&quot;643114&quot;, seg.length=30, bin.floor=50) ## NB: With series.x = FALSE (default), negative lags indicate missing rings in series Cross-correlations between series 643114 and the master chronology are shown for each segment (30-year segments lagged by 15 years). The series correlates well at lag 0 until the 1865–1894 bin and then at lag +1 prior to 1865. This figure shows that 1865 to 1894 is the misdated part of this series. The lag of +1 over a lag of 0 indicates that the series 643114 is missing a ring as it better correlates to the master chronology with a one-year offset.2 Using a smaller time window and shorter correlation segments we can try to further isolate the switch from correlation at lag 0 to lag +1. We will, of course, have to be very careful about using such short segments for correlation and be ready to adjust our expectations accordingly. Fortunately, in this case the trees are so exquisitely sensitive that we can look at 20-year segments with some confidence. win &lt;- 1850:1900 dat.win &lt;- subset(dat,dat.yrs %in% win) ccf.20 &lt;- ccf.series.rwl(rwl=dat.win, series=&quot;643114&quot;, seg.length=20, bin.floor=0) ## NB: With series.x = FALSE (default), negative lags indicate missing rings in series Cross-correlations between series `643114’ and the master chronology at 20-year segments lagged by 10 years over 1850–1900. By 1879 the correlation between series 643114 and the master is solidly at lag +1. The 1870 to 1889 correlation is marginal while the dating at 1880–1899 seems accurate (lag 0). This suggests that the dating error is between 1879 and 1889. At this point we could repeat the cross-correlation using even more carefuly to get even closer to the bad year if we wanted to. But at this point going and looking at the wood in that neighborhood would be the best option. Statistics only gets you so far. 6.8 Visual Crossdating We have strong inference now that series 643114 is misdated somewhere in a ten year period around 1885. One final tool that dplR has is the ability to combine the visual style of crossdating using skeleton plots with the statistical approach of cross-correlation analysis. The skel.ccf.plot function does just this. Here we make a skeleton plot from the 40-year period around the suspected dating error (1885): xskel.ccf.plot(rwl=dat, series=&quot;643114&quot;, win.start=1865, win.width=40) The top panel shows the normalized values for the master chronology (bottom half) and the series 643114 (top half) in green. The values are relative. Similarly, the black lines are a skeleton plot for the master and series with the marker years annotated for the master on the bottom axis and series 643114 on the top. The text at the top of the figure gives the correlation between the series and master (green bars) as well as the percentage of agreement between the skeleton bars for the series and master. The bottom panels show cross correlations for the first half (left) and second half of the time series. In this case, the early period (1865–1884) shows a mismatch of the skeleton plot by one year coupled with a strong lag +1 correlation. At this point the analyst would go to the wood and take a good look at the core and see what they could find out. There are more heroic efforts that one could go to to figure out exactly where the dating problem might be but nothing ever takes the place of looking at the sample! 6.9 Conclusion We have strong inference now that series 643114 is misdated somewhere in a ten year period around 1885. We have still not revealed whether this is correct or not. Let us look at the values for i and j and see how we did: j ## [1] 13 colnames(co021)[j] ## [1] &quot;643143&quot; i ## [1] 557 rownames(co021)[i] ## [1] &quot;1732&quot; Our sleuthing indicated that our dating error was around the year 1885. In fact, i was the year 1884. As of dplR version 1.60, the cross correlations in ccf.series.rwl are calculated calling ccf(x=series, y=master, lag.max=lag.max, plot=FALSE). Note that prior to dplR version 1.60, the master was set as x and the series as y. This was changed to be more in line with user expectations so that a missing ring in a series produces a positive lag in the plot rather than a negative lag. This structure of this call does put the plots at odds with Figure 3 in (2010) which is unfortunate.↩︎ "],["intro-to-dendro-time-series.html", "Chapter 7 Intro to dendro time-series 7.1 Introduction 7.2 Data Sets 7.3 Smoothing 7.4 Characterizing Temporal Structure 7.5 Frequency Domain", " Chapter 7 Intro to dendro time-series In this document we cover some of the basic time series tools in dplR. These include spectral analysis using redfit and wavelets. We also show off a few tools in other R packages like fitting AR and ARMA models, and Butterworth filters. However, this is just a tiny glimpse of tree-ring tools that exist in R. 7.1 Introduction The Dendrochronology Program Library in R (dplR) is a package for dendrochronologists to handle data processing and analysis. This document gives an introduction of some of the functions dealing with time series in dplR and looks at some functions other packages. However, this document does not purport to be any sort of authority on time series analysis at all! There are many wonderful R-centric books on time series analysis that can tell you about the theory and practice of working with time-series data. This is kind of a greatest hits of time-series tools that are commonly used in dendro, and not a reference text of any kind. 7.1.1 Load libraries We will be using dplR in here as well as a few other packages that are useful for working with time-series data. If you haven’t installed these packages yet you will want to using the install.packages() function. library(dplR) ## This is dplR version 1.7.4. ## dplR is part of openDendro https://opendendro.org. ## New users can visit https://opendendro.github.io/dplR-workshop/ to get started. 7.2 Data Sets Throughout this document we will use the on-board data set co021 which gives the raw ring widths for Douglas fir Pseudotsuga menziesii at Mesa Verde in Colorado, USA. There are 35 series spanning 788 years. data(co021) co021.sum &lt;- summary(co021) head(co021.sum) ## series first last year mean median stdev skew gini ar1 ## 1 641114 1270 1963 694 0.287 0.23 0.231 2.884 0.372 0.686 ## 2 641121 1250 1963 714 0.328 0.26 0.315 3.306 0.410 0.744 ## 3 641132 1256 1963 708 0.357 0.29 0.337 4.741 0.373 0.686 ## 4 641143 1237 1963 727 0.344 0.27 0.287 2.341 0.397 0.708 ## 5 642114 1243 1963 721 0.281 0.24 0.219 2.848 0.358 0.673 ## 6 642121 1260 1963 704 0.313 0.21 0.416 4.399 0.474 0.865 plot(co021, plot.type=&quot;spag&quot;) These data are gorgeous with long segments (564), high interseries correlation (0.6), and high first-order autocorrelation (0.85). Let us make a mean-value chronology of the co021 data after detrending each series with a frequency response of 50% at a wavelength of 2/3 of each series’s length. co021.rwi &lt;- detrend(co021, method=&quot;Spline&quot;) co021.crn &lt;- chron(co021.rwi) We can plot the chronology with a smoothing spline with a 20-year period: plot(co021.crn, add.spline=TRUE, nyrs=20) 7.3 Smoothing Very often one wants to smooth data to analyze or work only with particular aspects of the data. This kind of analysis is a big deal in time-series analysis. Smoothing is a way of isolating different frequencies of data but still thinking in the time domain. One such mark that we already alluded to above is the use of smoothing splines to detrend and filter tree-ring data. The urge to smooth a time series when we plot it is almost irresistible. It’s a sickness that dendro folks have. There are many different ways of smoothing and we will look at couple here. The co021.crn object has two columns, the first giving the chronology and the second the sample depth during that year. Let’s grab the years and the standard chronology and store them in their own objects. yrs &lt;- time(co021.crn) dat &lt;- co021.crn$std 7.3.1 Moving (running) average Centered moving averages (aka running averages) tend to be the first smoothing method that most people learn. Let’s revisit them. These have the advantage of being dirt simple. Below we will see that they emphasize low frequency in the examples below (at 32, 64, and 128 years) but retain some jaggedness too. We will use the base filter function which is a straightforward convolution filter. As with any function always you can see the help file for details (?filter). ma32 &lt;- filter(x=dat, filter=rep(x=1/32,times=32), sides=2) ma64 &lt;- filter(x=dat, filter=rep(x=1/64,times=64), sides=2) ma128 &lt;- filter(x=dat, filter=rep(x=1/128,times=128), sides=2) # Change plotting parameters par(mar=rep(2.5,4),mgp=c(1.2,0.25,0),tcl=0.5, xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) my.cols &lt;- c(&quot;#1B9E77&quot;, &quot;#D95F02&quot;, &quot;#7570B3&quot;) plot(yrs,dat,type=&quot;l&quot;,xlab=&quot;Year&quot;,ylab=&quot;RWI&quot;,col=&quot;grey&quot;) abline(h=1) lines(yrs,ma128, col = my.cols[1], lwd = 2) lines(yrs,ma64, col = my.cols[2], lwd = 2) lines(yrs,ma32, col = my.cols[3], lwd = 2) axis(3);axis(4) 7.3.2 Hanning The Hanning filter is similar to the moving average in the sense that the curve emphasizes low frequency variability and loses the jaggedness over the moving average. It’s also a simple filter (look at the code by typing hanning at the R prompt) but it also is a start into thinking in the frequency domain. It’s part of a family of functions called window functions that are zero-valued outside of some interval chosen by the user. It’s used quite a bit by time-series wonks and it is implemented in dplR with the function hanning. I’ll skip the theory here but it’s a great precursor to the work we will do with spectral analysis below. han32 &lt;- hanning(dat,n=32) han64 &lt;- hanning(dat,n=64) han128 &lt;- hanning(dat,n=128) par(mar=rep(2.5,4),mgp=c(1.2,0.25,0),tcl=0.5, xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) plot(yrs,dat,type=&quot;l&quot;,xlab=&quot;Year&quot;,ylab=&quot;RWI&quot;,col=&quot;grey&quot;) abline(h=1) lines(yrs,han128, col = my.cols[1], lwd = 2) lines(yrs,han64, col = my.cols[2], lwd = 2) lines(yrs,han32, col = my.cols[3], lwd = 2) axis(3);axis(4) Like the moving average the smooth is shorter than the input data. This is too bad because it is nice to preserve the ends of the data but there is much wringing of hands and gnashing of teeth about the “right” way of running a smoothing algorithm where the data are sparse. This end-member problem is the subject of a lot of work in the time-series literature. head(data.frame(dat,ma32, han32), n = 20) ## dat ma32 han32 ## 1 1.0644 NA NA ## 2 0.9241 NA NA ## 3 0.9602 NA NA ## 4 0.8011 NA NA ## 5 1.2448 NA NA ## 6 1.3222 NA NA ## 7 0.4193 NA NA ## 8 0.7125 NA NA ## 9 1.0207 NA NA ## 10 0.9851 NA NA ## 11 0.4199 NA NA ## 12 0.7638 NA NA ## 13 1.4655 NA NA ## 14 1.0005 NA NA ## 15 1.0725 NA NA ## 16 0.4987 0.9226 0.9609 ## 17 0.6365 0.9167 0.9648 ## 18 0.7509 0.9147 0.9667 ## 19 2.0863 0.9205 0.9665 ## 20 1.2892 0.9464 0.9644 tail(data.frame(dat,ma32, han32), n = 20) ## dat ma32 han32 ## 769 1.0528 0.9214 0.9836 ## 770 0.9065 0.9094 0.9761 ## 771 0.4347 0.8989 0.9660 ## 772 1.1876 0.8883 0.9533 ## 773 1.1743 NA NA ## 774 1.7610 NA NA ## 775 0.6408 NA NA ## 776 0.1934 NA NA ## 777 1.3441 NA NA ## 778 0.4973 NA NA ## 779 0.7076 NA NA ## 780 0.7565 NA NA ## 781 0.5028 NA NA ## 782 1.0647 NA NA ## 783 1.1284 NA NA ## 784 0.1459 NA NA ## 785 1.1805 NA NA ## 786 0.7720 NA NA ## 787 0.6233 NA NA ## 788 0.6441 NA NA 7.3.3 Splines In the world of tree rings, you’ll see frequent mention of cubic smoothing splines and typically a citation for something by Ed Cook who is the greatest quantitative dendrochronologist of all time. His work has left an enduring mark on nearly every aspect of quantitative dendrochronology. For splines, Cook and Peters (1981) is the canonical citation but we would point you to Cook and Kairiukstis (Cook et al., 1990) for an overview. An R implementation of this spline is implemented in dplR with the ffcsaps function which was written in R and also in the caps function which is a wrapper for Cook’s Fortran code. They produce (nearly) identical results. Because caps is quite a bit faster than ffcsaps, dplR uses it by default. The important argument for users is the nyrs argument which fits a spline of that many “years” to the data. All the functions in dplR assume annual resolution but they don’t really care if you are using data with a different frequency. spl128 &lt;- caps(dat,nyrs=128) spl64 &lt;- caps(dat,nyrs=64) spl32 &lt;- caps(dat,nyrs=32) par(mar=rep(2.5,4),mgp=c(1.2,0.25,0),tcl=0.5, xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) my.cols &lt;- c(&quot;#1B9E77&quot;, &quot;#D95F02&quot;, &quot;#7570B3&quot;) plot(yrs,dat,type=&quot;n&quot;,xlab=&quot;Year&quot;,ylab=&quot;RWI&quot;,axes=FALSE) grid(col=&quot;black&quot;,lwd=0.5) abline(h=1) lines(yrs,dat,col=&quot;grey&quot;,lwd=1) lines(yrs,spl128,col=my.cols[1],lwd=2) lines(yrs,spl64,col=my.cols[2],lwd=2) lines(yrs,spl32,col=my.cols[3],lwd=2) axis(1);axis(2);axis(3);axis(4) legend(&quot;topright&quot;, c(&quot;dat&quot;, &quot;128yrs&quot;, &quot;64yrs&quot;, &quot;32yrs&quot;), lwd = 2, col = c(&quot;grey&quot;,my.cols),bg = &quot;white&quot;) box() 7.3.4 Loess We will highlight a filter used commonly in time-series analysis but largely ignored in dendro. We’ve become increasingly fond of the loess smoother which uses a local, linear polynomial fit to smooth data. The parameter to adjust is the span f. Because f is the proportion of points used in the smoothing, the smaller the number the less smooth the curve will be. E.g., our series is 788 years long. If we want something that approaches a 25-year smooth we can take 25 over 788 and use 0.0317 of the points in the fit. n &lt;- length(yrs) f128 &lt;- 128/n f128.lo &lt;- lowess(x = yrs, y = dat, f = f128) f64 &lt;- 64/n f64.lo &lt;- lowess(x = yrs, y = dat, f = f64) f32 &lt;- 32/n f32.lo &lt;- lowess(x = yrs, y = dat, f = f32) par(mar=rep(2.5,4),mgp=c(1.2,0.25,0),tcl=0.5,xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) plot(yrs,dat,type=&quot;n&quot;,xlab=&quot;Year&quot;,ylab=&quot;RWI&quot;,axes=FALSE) grid(col=&quot;black&quot;,lwd=0.5) abline(h=1) lines(yrs,dat,col=&quot;grey&quot;,lwd=1) lines(yrs,f128.lo$y,col=my.cols[1],lwd=2) lines(yrs,f64.lo$y,col=my.cols[2],lwd=2) lines(yrs,f32.lo$y,col=my.cols[3],lwd=2) axis(1);axis(2);axis(3);axis(4) legend(&quot;topright&quot;, c(&quot;dat&quot;, &quot;f128&quot;, &quot;f64&quot;, &quot;f32&quot;), lwd = 2, col = c(&quot;grey&quot;,my.cols),bg = &quot;white&quot;) box() These smoothing lines above are essentially eye candy. We can and will use filtering for more sophisticated analysis below but let’s back up. 7.4 Characterizing Temporal Structure 7.4.1 ACF and PACF Let’s start with a quick exploratory data analysis into the time-series process. We will start our analysis on the chronology by looking at its autocorrelation structure using R’s acf and pacf functions. par(mfcol=c(1, 2)) acf(dat) pacf(dat) The ACF function indicates significant autocorrelation out to a lag of about 10 years (which is not uncommon in tree-ring data) while the PACF plot suggests that the persistence after lag 4 is due to the propagation of the autocorrelation at earlier lags. And one could very well argue that the best model here is an AR(2) model given the low value of the PACF at lags 3 and 4. After all, you can get three opinions by asking one statistician to look a time series. But we digress. We now have the first bit of solid information about the time-series properties of these data, it looks like they fit an AR(4) model. 7.4.2 AR and ARMA But, R being R, there are many other ways to check this. The easiest way is to use the ar function which fits an autoregressive models up to a specified order and selects the final model by AIC. dat.ar &lt;- ar(dat,order.max = 10) dat.ar ## ## Call: ## ar(x = dat, order.max = 10) ## ## Coefficients: ## 1 2 3 4 ## 0.200 0.148 0.046 0.075 ## ## Order selected 4 sigma^2 estimated as 0.188 Indeed, ar selects an AR(4) model based on AIC. But as our tea-leaf gazing above indicates, an AR(2) model is very close when we look at the AIC values. plot(0:10,dat.ar$aic,type=&quot;b&quot;,xlab=&quot;AR Order&quot;,ylab=&quot;AIC&quot;, main=&quot;Difference in AIC between each model and the best-fitting model&quot;) A strong argument can be made that a model with fewer parameters (e.g., an AR(2) might serve to characterize the data better than a model with more parameters. We could also fit these models as ARIMA models individually using the arima function. That function will fit a model with components (p, d, q) as the AR order, the degree of difference, and the MA order. Thus, we could fit AR(1) to AR(4) models: ar1 &lt;- arima(dat,order=c(1,0,0)) ar2 &lt;- arima(dat,order=c(2,0,0)) ar3 &lt;- arima(dat,order=c(3,0,0)) ar4 &lt;- arima(dat,order=c(4,0,0)) # example output ar1 ## ## Call: ## arima(x = dat, order = c(1, 0, 0)) ## ## Coefficients: ## ar1 intercept ## 0.260 0.975 ## s.e. 0.034 0.021 ## ## sigma^2 estimated as 0.195: log likelihood = -473.7, aic = 953.3 And then compare the Bayesian Information Criterion (BIC) values: BIC(ar1,ar2,ar3,ar4) ## df BIC ## ar1 3 967.3 ## ar2 4 949.9 ## ar3 5 953.5 ## ar4 6 955.8 Here we find evidence for an AR(2) model over the AR(4) mode that the ar function chose. Clearly there is some wiggle room here. So as we’ve seen, in addition to AR models we can fit their more complicated cousins, ARIMA models, as well. We can do the same sort of automatic selection of the model order by automatically fitting an ARIMA model using the auto.arima function in the package forecast. As with ar, this will choose a model that minimizes an information criterion after searching over user given constraints. We will use the default search parameters and use BIC to select the model. library(forecast) ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo dat.arima &lt;- auto.arima(dat, ic=&quot;bic&quot;) summary(dat.arima) ## Series: dat ## ARIMA(1,0,1) with non-zero mean ## ## Coefficients: ## ar1 ma1 mean ## 0.827 -0.634 0.974 ## s.e. 0.050 0.068 0.032 ## ## sigma^2 = 0.187: log likelihood = -457.1 ## AIC=922.2 AICc=922.3 BIC=940.9 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 7.679e-05 0.4322 0.3427 -457.9 481.3 0.7942 -0.0004114 Even more confusion! Instead of an AR(p) model, auto.arima went for an ARMA(1,1) model (aka ARIMA(1,0,1)). The parsimony principle certainly likes a nice simple ARMA(1,1) model. Note that we could look at the residuals, model coefficients, etc. quite easily. And indeed the residuals are quite clean as we would expect. acf(residuals(dat.arima)) This is clearly an incredibly shallow look at a deep topic. Hopefully these will get you started with different tools to characterize temporal structure in data. There are many more resources available and techniques we haven’t covered (e.g., GARCH modelling). 7.5 Frequency Domain Any time series can be broken down into a (possibly infinite) set of sine and cosine functions. So, a messy looking series like co021.crn is the sum of oscillating functions. The act of breaking these down is exactly what a Fourier transform does. Fourier transforms are everywhere – think JPEGs and MP3s. But we can use them to see what frequencies are important or dominant in a time series. Someday we will work up the courage to write a tutorial on fast Fourier transforms for dendro using R. It’s hard to know where to start with tackling the subject though as many of the tools that people want for dendro start with fft(dat) but don’t end there. With tree-ring data we often use spectral analysis as a way of detecting periodic signals that are corrupted by noise. 7.5.1 Power via Spectral Analysis and Wavelets Given the above rationale, we will limit ourselves here to some of the tools that are used in dendro without building these up from the fast Fourier transform itself. In dplR, we’ve implemented two of the most common ways that dendrochronologists go about this and there are a host of other approaches in R that we won’t get to here. The redfit function in dplR is a port of Schulz’s REDFIT (version 3.8e) program and estimates the red-noise spectrum of a time series (Schulz &amp; Mudelsee, 2002) with optional testing of that spectrum against a red-noise background using Monte Carlo simulations. redf.dat &lt;- redfit(dat, nsim = 1000) par(tcl = 0.5, mar = rep(2.2, 4), mgp = c(1.1, 0.1, 0),xaxs=&quot;i&quot;) plot(redf.dat[[&quot;freq&quot;]], redf.dat[[&quot;gxxc&quot;]], ylim = range(redf.dat[[&quot;ci99&quot;]], redf.dat[[&quot;gxxc&quot;]]), type = &quot;n&quot;, ylab = &quot;Spectrum&quot;, xlab = &quot;Frequency (cycles per year)&quot;, axes = FALSE) grid() lines(redf.dat[[&quot;freq&quot;]], redf.dat[[&quot;gxxc&quot;]], col = &quot;black&quot;,lwd=1.5) lines(redf.dat[[&quot;freq&quot;]], smooth.spline(redf.dat[[&quot;ci99&quot;]],spar = 0.8)$y, col = &quot;#D95F02&quot;) lines(redf.dat[[&quot;freq&quot;]], smooth.spline(redf.dat[[&quot;ci95&quot;]],spar = 0.8)$y, col = &quot;#7570B3&quot;) lines(redf.dat[[&quot;freq&quot;]], smooth.spline(redf.dat[[&quot;ci90&quot;]],spar = 0.8)$y, col = &quot;#E7298A&quot;) freqs &lt;- pretty(redf.dat[[&quot;freq&quot;]]) pers &lt;- round(1 / freqs, 2) axis(1, at = freqs, labels = TRUE) axis(3, at = freqs, labels = pers) mtext(text = &quot;Period (year)&quot;, side = 3, line = 1.1) axis(2); axis(4) legend(&quot;topright&quot;, c(&quot;dat&quot;, &quot;CI99&quot;, &quot;CI95&quot;, &quot;CI90&quot;), lwd = 2, col = c(&quot;black&quot;, &quot;#D95F02&quot;, &quot;#7570B3&quot;, &quot;#E7298A&quot;), bg = &quot;white&quot;) box() Using the Mesa Verde chronology we see that there are frequencies in that time series that are significantly different from a red-noise assumption in the interannual (&lt;3 years) and at low frequencies (multidecadal). Another popular way to visualize a tree-ring chronology in the frequency domain is through a continuous wavelet transform. In dplR, there is are functions for calculating the transform via wavelet and plotting the result via wavelet.plot. out.wave &lt;- morlet(y1 = dat, x1 = yrs, p2 = 8, dj = 0.1, siglvl = 0.99) wavelet.plot(out.wave, useRaster=NA, reverse.y = TRUE) The wavelet plot shows a similar story as the plot from redfit with significant variation at interannual to multidecadal scales. 7.5.2 Extracting signals Another common task we’ll mention here is extracting specific frequency components from a time series to look at different aspects of say, high vs low frequency growth. One approach to doing this is to use wavelets again but here we will decompose a time series into its constituent voices using a discrete wavelet transform with the mra function in the package waveslim. library(waveslim) ## ## waveslim: Wavelet Method for 1/2/3D Signals (version = 1.8.2) nPwrs2 &lt;- trunc(log(n)/log(2)) - 1 dat.mra &lt;- mra(dat, wf = &quot;la8&quot;, J = nPwrs2, method = &quot;modwt&quot;, boundary = &quot;periodic&quot;) yrsLabels &lt;- paste(2^(1:nPwrs2),&quot;yrs&quot;,sep=&quot;&quot;) par(mar=c(3,2,2,2),mgp=c(1.25,0.25,0),tcl=0.5,xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) plot(yrs,rep(1,n),type=&quot;n&quot;, axes=FALSE, ylab=&quot;&quot;,xlab=&quot;&quot;, ylim=c(-3,38)) title(main=&quot;Multiresolution decomposition&quot;,line=0.75) axis(side=1) mtext(&quot;Years&quot;,side=1,line = 1.25) Offset &lt;- 0 dat.mra2 &lt;- scale(as.data.frame(dat.mra)) for(i in nPwrs2:1){ x &lt;- dat.mra2[,i] + Offset lines(yrs,x) abline(h=Offset,lty=&quot;dashed&quot;) mtext(names(dat.mra)[[i]],side=2,at=Offset,line = 0) mtext(yrsLabels[i],side=4,at=Offset,line = 0) Offset &lt;- Offset+5 } box() Here the Mesa Verde chronology is shown via an additive decomposition for each power of 2 from \\(2^1\\) to \\(2^8\\). Note that each voice is scaled to itself by dividing by its standard deviation in order to present them on the same y-axis. If the scale function were to be removed (and we leave that as an exercise to the reader) the variations between voices would be greatly reduced. Note the similarity in the continuous and discrete wavelet plots for the variation in the 64-year band around the year 1600 and the lower frequency variation at 128 years around the year 1400. 7.5.3 High-pass, low-pass, and bandpass filtering Another common technique used in tree-ring analysis is to filter data like we did above with moving averages, splines, and so on but doing so in the frequency domain explicitly rather than in the time domain. For instance, we might want to remove the high frequencies from a data set and leave the low frequencies (low-pass filtering) or vice versa (high-pass filtering). Or extract specific frequencies and discard others (band-pass and stop-pass filtering). Let’s look at the signal library.3 library(signal) ## ## Attaching package: &#39;signal&#39; ## The following object is masked from &#39;package:dplR&#39;: ## ## hanning ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, poly We can start low-pass filtering in R with the Butterworth filter. We will fit two 4th order filters with frequencies of 0.2 and 0.05. # here are two frequencies that we will use for demonstrating filtering f1 &lt;- 0.2 # period of 5 f2 &lt;- 0.05 # period of 20 # Initialize the butterworth filter. # Multiply freq by 2 f1Low &lt;- butter(n=4, W=f1*2, type=&quot;low&quot;) f2Low &lt;- butter(n=4, W=f2*2, type=&quot;low&quot;) # We will use filtfilt do run the filter. # But before that there is a wrinkle. We have to # mirror and extend the data near the edge to # avoid end effects. The matlab/octave versions do something # like this behind the scenes I think. Regardless, it # is a simple form of padding. n &lt;- length(dat) # create a pas that is adds a 1/4 of the length to the front and # back of the data. This can be tuned according to the freqs in question # and there is nothing special about 1/4 pad &lt;- floor(n/4) # pad the data datPad &lt;- c(dat[pad:1],dat,dat[n:(n-pad)]) # run the filter datF1Low &lt;- filtfilt(f1Low, datPad) datF2Low &lt;- filtfilt(f2Low, datPad) # unpad the filtered data datF1Low &lt;- datF1Low[(pad+1):(n+pad)] datF2Low &lt;- datF2Low[(pad+1):(n+pad)] par(mar=rep(2.5,4),mgp=c(1.2,0.25,0),tcl=0.5,xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) plot(yrs,dat,type=&quot;n&quot;,xlab=&quot;Year&quot;,ylab=&quot;&quot;,axes=FALSE) grid(col=&quot;black&quot;,lwd=0.5) abline(h=1) lines(yrs,dat,col=&quot;grey80&quot;,lwd=1) lines(yrs,datF1Low,col=my.cols[1],lwd=1.5) lines(yrs,datF2Low,col=my.cols[2],lwd=2) axis(1);axis(2);axis(3);axis(4) legend(&quot;topright&quot;, c(&quot;f &lt; 0.2&quot;, &quot;f &lt; 0.05&quot;), lwd = 2, col = c(my.cols[1:2]),bg = &quot;white&quot;) box() To satisfy ourselves that the low-pass filter did what we thought it should let’s look at the power spectrum of the filtered data for f1: redf.dat &lt;- redfit(datF1Low, mctest=FALSE) par(tcl = 0.5, mar = rep(2.2, 4), mgp = c(1.1, 0.1, 0),xaxs=&quot;i&quot;) plot(redf.dat[[&quot;freq&quot;]], redf.dat[[&quot;gxxc&quot;]], xlim=c(0,0.25), type = &quot;n&quot;, ylab = &quot;Spectrum&quot;, xlab = &quot;Frequency (cycles per year)&quot;, axes = FALSE) grid() lines(redf.dat[[&quot;freq&quot;]], redf.dat[[&quot;gxxc&quot;]], col = &quot;black&quot;,lwd=1.5) freqs &lt;- pretty(redf.dat[[&quot;freq&quot;]]) pers &lt;- round(1 / freqs, 2) axis(1, at = freqs, labels = TRUE) axis(3, at = freqs, labels = pers) mtext(text = &quot;Period (year)&quot;, side = 3, line = 1.1) axis(2); axis(4) box() We could repeat this with high-pass filters by changing type=\"high\" when we first make the filter. We could also filter to include only specified frequencies by providing the lower and upper bands. E.g., butter(n=4, W=c(f2,f1)*2, type=\"pass\"). f1f2Pass &lt;- butter(n=4, W=c(f2,f1)*2, type=&quot;pass&quot;) datPass &lt;- filtfilt(f1f2Pass, datPad) datPass &lt;- datPass[(pad+1):(n+pad)] par(mar=rep(2.5,4),mgp=c(1.2,0.25,0),tcl=0.5,xaxs=&quot;i&quot;,yaxs=&quot;i&quot;) plot(yrs,datPass,type=&quot;n&quot;,xlab=&quot;Year&quot;,ylab=&quot;&quot;,axes=FALSE) grid(col=&quot;black&quot;,lwd=0.5) abline(h=0) lines(yrs,datPass,col=my.cols[3],lwd=1.5) axis(1);axis(2);axis(3);axis(4) legend(&quot;topright&quot;, &quot;0.05 &lt; f &lt; 0.2&quot;, lwd = 2, col = c(my.cols[3]),bg = &quot;white&quot;) box() And as above, here is the power spectrum of the band-pass data: redf.dat &lt;- redfit(datPass, mctest=FALSE) par(tcl = 0.5, mar = rep(2.2, 4), mgp = c(1.1, 0.1, 0),xaxs=&quot;i&quot;) plot(redf.dat[[&quot;freq&quot;]], redf.dat[[&quot;gxxc&quot;]], xlim=c(0,0.25), type = &quot;n&quot;, ylab = &quot;Spectrum&quot;, xlab = &quot;Frequency (cycles per year)&quot;, axes = FALSE) grid() lines(redf.dat[[&quot;freq&quot;]], redf.dat[[&quot;gxxc&quot;]], col = &quot;black&quot;,lwd=1.5) freqs &lt;- pretty(redf.dat[[&quot;freq&quot;]]) pers &lt;- round(1 / freqs, 2) axis(1, at = freqs, labels = TRUE) axis(3, at = freqs, labels = pers) mtext(text = &quot;Period (year)&quot;, side = 3, line = 1.1) axis(2); axis(4) box() 7.5.4 Interruption: Duplicate function names Take a moment to look back at the messages given off from loading signal. There are times when two packages will have the same function names. For instance both the signal package and dplR have a function called hanning (which applies the hanning filter to a time series). When two libraries are loaded that have two functions with the same name, R has to decide which to use when the function is called from the command line. Thus if we type hanning at the prompt we will get the `hanning function from the signal library. Because signal was loaded after dplR the hanning function from signal gets primacy. The same goes for the signal functions filter and poly which have counterparts the library stats which is loaded when R is first started. This dual name phenomenon can occasionally (but only occasionally) be a hindrance if you forget that it has happened. For instance, the function hanning in signal doesn’t do the same thing as the function hanning does in dplR. If you were expecting the behavior of one and got the other you might break your scripts, confuse yourself, and spend time tracking down the error. So, pay attention to masking when you load a package. We can still access dplR’s version of hanning via dplR::hanning if you want it. The same goes for stats::filter, stats::poly, and so on. But note the information on name masking that gets spit out when we do – see explanation in the next section.↩︎ "],["acknowledgements.html", "Chapter 8 Acknowledgements", " Chapter 8 Acknowledgements I started making dplR in 2007 or so. But it would look nothing like it does today without the contributions of many collaborators over the years. I would especially like to thank Mikko Korpela. I have never met Mikko in person but he spent countless hours (but into the thousands I think) over several years taking my ugly R and returning much prettier and efficient code. In 2022 we made significant upgrades to dplR thanks to a grant from the National Science Foundation (Award 2054516) which allowed us to do several things but the most important was that we locked Ed Cook in a conference room until he translated some of the finer aspects of his ARSTAN program. There are a great many other collaborators who have helped over the years contributing code and making bug fixes. There would be no dplR without these folks. Buy them a nice beverage when you meet them. And if you get to meet Mikko, buy him whatever he might ask for. Person Role(s) Andy Bunn Author, Copyright holder, Creator, Translator to R Mikko Korpela Author, Copyright holder, Translator to R Franco Biondi Author, Copyright holder Filipe Campelo Author, Copyright holder Pierre Mérian Author, Copyright holder Fares Qeadan Author, Copyright holder Christian Zang Author, Copyright holder Allan Buras Contributor Alice Cecile Contributor Manfred Mudelsee Contributor Michael Schulz Contributor Stefan Klesse Contributor David Frank Contributor Ronald Visser Contributor Ed Cook Contributor Kevin Anchukaitis Contributor Thanks to J Zobolas, creator of rtemps, for the template used here. "],["references.html", "Chapter 9 References", " Chapter 9 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
